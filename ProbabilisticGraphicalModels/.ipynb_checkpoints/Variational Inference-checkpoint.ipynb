{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on MLPP's chap 12: Variational Inference:\n",
    "- \n",
    "\n",
    "Notes\n",
    "- I'm changing the notation on $P$ here for a markov network to be $P_{\\Phi}$\n",
    "\n",
    "Notes from PGM on Mean Field\n",
    "- using a product of marginals makes inference easier. We need only consider the marginals involving our query variables.\n",
    "\n",
    "What needs to be mentioned?\n",
    "\n",
    "- Parameters can be viewed as hidden variables..\n",
    "- I haven't even mentioned hidden variables yet!\n",
    "- How does conditioning on $\\mathbf{x}$ relate to conditioning on $D$ (think of many Gibbs table and you average their answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PGM: Sec 11.5.1 The Mean Field Approximation\n",
    "\n",
    "- The algo performs message passing where the messages are distributions over single cariables\n",
    "- It finds the distribution that is closest to $P_\\Phi$ and is only a factor of marginals.\n",
    "- It's computational attractive because we only need to consider the marginals of the variables involved in our query.\n",
    "- This is the 'energy functional': $\\mathbb{H}[\\textrm{Q}] - \\mathbb{H}[\\textrm{Q},\\tilde{P}_\\Phi]$. We are maximizes it when we reduce KL divergence. It approaches $\\log Z$.\n",
    "- Consider each term. Consider trying to make $ - \\mathbb{H}[\\textrm{Q},\\tilde{P}_\\Phi]$ large. This is just:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " - \\mathbb{H}[\\textrm{Q},\\tilde{P}_\\Phi] & = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\log \\tilde{P}_\\Phi(\\mathbf{x}) \\\\\n",
    " & = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\big[\\log \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i) \\big]\\\\\n",
    "  & = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\big[ \\sum_{\\phi_i\\in \\Phi} \\log \\phi_i(\\mathbf{D}_i) \\big]\\\\\n",
    "  & = \\sum_{\\phi_i\\in \\Phi} \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x})  \\log \\phi_i(\\mathbf{D}_i) \\\\\n",
    "  & = \\sum_{\\phi_i\\in \\Phi} \\mathbb{E}_{\\mathbf{D}_i\\sim \\textrm{Q}}\\big[\\log \\phi_i(\\mathbf{D}_i) \\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\mathbb{H}[\\textrm{Q}]$ also decomposes. It decomposes into a sum of the marginal entropies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Variance Inference?\n",
    "\n",
    "(This is the 3rd answer in a 7 part series[link] on Probabilistic Graphical Models ('PGMs').)\n",
    "\n",
    "We've defined inference as:\n",
    "\n",
    "\"The task of using a given graphical model of a system, complete with fitted parameters, to answer certain questions regarding that system.\"\n",
    "\n",
    "'Variational Inference' ('VI') relies on tricks from information theory to do this *approximately* - a necessary consession when the exact alternative is hopelessly expensive. It involves recasting inference as an optimization problem, which offers an altnerative view of the task and unexpectedly, an extremely useful by product quantity.\n",
    "\n",
    "Now, let's recap all that is necessary to understand this answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher (similar to refresher in answer 2)\n",
    "\n",
    "In the first answer [link], we discovered why PGMs are useful tools for representing complex system. We defined a complex system as a set of $n$ random variables (which we call $\\mathcal{X}$) with a relationship we'd like to understand. We take it that there exists some true but unknown joint distribution, $P$, which govern these RVs. We take it that a 'good understanding' means we can answer two types of questions regarding this $P$:\n",
    "\n",
    "1. **Probability Queries**: Compute the probabilities $P(\\mathbf{Y}|\\mathbf{e})$. What is the distribution of the RV's of $\\mathbf{Y}$ given we have some observation ($\\mathbf{e}$) of the RVs of $\\mathbf{E}$?\n",
    "2. **MAP Queries**: Determine $\\textrm{argmax}_\\mathbf{Y}P(\\mathbf{Y}|\\mathbf{e})$. That is, determine the most likely values of some RVs given an assignment of other RVs.\n",
    "\n",
    "(Where $\\mathbf{E}$ and $\\mathbf{Y}$ are two arbitrary subsets of $\\mathcal{X}$. If this notation is unfamiliar, see the 'Notation Guide' section from the first answer [link]).\n",
    "\n",
    "The idea behind PGMs is to estimate $P$ using two things:\n",
    "\n",
    "1. A graph: a set of nodes, each of which represents an RV from $\\mathcal{X}$, and a set of edges between these nodes.\n",
    "2. Parameters: objects that, when paired with a graph and a certain rule, allow us to calculate probabilities of assignments of $\\mathcal{X}$.\n",
    "\n",
    "PGMs fall into two main categories, Bayesian Networks ('BNs') and Markov Networks ('MNs'), depending on the specifics of these two.\n",
    "\n",
    "A **Bayesian Network** involves a graph, denoted as $\\mathcal{G}$, with *directed* edges and no directed cycles. The parameters are Conditional Probability Tables ('CPDs' or 'CPTs'), which are, as the naming suggests, select conditional probabilities from the BN. They give us the right hand side of the Chain Rule, which dictates we calculate probabilities according to a BN:\n",
    "\n",
    "$$\n",
    "P_{B}(X_1,\\cdots,X_n)=\\prod_{i=1}^n P_{B}(X_i|\\textrm{Pa}_{X_i}^\\mathcal{G})\n",
    "$$\n",
    "\n",
    "where $\\textrm{Pa}_{X_i}^\\mathcal{G}$ indicates the set of parents nodes/RVs of $X_i$ according to $\\mathcal{G}$.\n",
    "\n",
    "A **Markov Network**'s graph, denoted as $\\mathcal{H}$, is different in that it's edges are *undirected* and we may have cycles. The parameters are a size $m$ set of *functions* which map assignments of $m$ subsets of $\\mathcal{X}$ to nonnegative numbers. Those subsets, which we'll call $\\mathbf{D}_i$'s, correspond to *complete subgraphs* of $\\mathcal{H}$ and their union makes up the whole of $\\mathcal{H}$. We can refer to this set as $\\Phi=\\{\\phi_i(\\cdots)\\}_{i=1}^m$. With that, we say that the 'Gibbs Rule' for calculation probabilities is:\n",
    "\n",
    "$$\n",
    "P_M(X_1,\\cdots,X_n) = \\frac{1}{Z} \\underbrace{\\prod_{i = 1}^m \\phi_i(\\mathbf{D}_i)}_{\\text{we call this }\\tilde{P}_M(X_1,\\cdots,X_n)}\n",
    "$$\n",
    "\n",
    "where $Z$ is a normalizer - it ensures our probabilities sum to 1.\n",
    "\n",
    "To crystallize this idea, it's helpful to imagine the 'Gibbs Table', which lists unnormalized probabilities for all assignments. In the second answer [link], we pictured an example where $\\mathcal{X}=\\{C,D,I,G,S\\}$ as:\n",
    "\n",
    "![Title](GIbbsTable_labeled.png)\n",
    "\n",
    "Lastly, we recall that the Gibbs Rule may expression the Chain Rule. That is, we can always recreate the probabilities produced by a BN's Chain Rule with an another invented MN and its Gibbs Rule. Essentially, we define factors as those that reproduce looking up a conditional probability in a BN's CPDs. This equivalence allows us to reason solely in terms of the Gibbs Rule, while assured that whatever we discover will also hold for BNs. In other words, with regards to inference, if something works for $P_M$, then it works for $P_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The big problem\n",
    "\n",
    "The issue is that the Gibbs Table may have exponentially many rows (assignments) and we can't sum effeciently across them. So that means we can't calculate $Z$ for starters. Exact inference algorithms help, but their 'exact' constraint puts significant limits on their speed. There are some graphs for which exact inference just isn't an option.\n",
    "\n",
    "So we need to be clever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The big idea\n",
    "\n",
    "The idea behind VI is to consider a space of 'simple' probability distributions and pick one that closely approximates $P_M$. With one selected, we can perform inference on this distribution rather than $P_M$. The 'simple' constraint ensures such inference is feasible. To do this, we need to determine this space, have a method to search it and have a measure that tells us how well one distribution approximates another.\n",
    "\n",
    "Let's say that with notation. Let $\\mathcal{Q}$ be a space of distribution and let $\\textrm{Q}$ refer to any distribution from $\\mathcal{Q}$. We want to answer the optimization of:\n",
    "\n",
    "$$\n",
    "\\textrm{Q}^* = \\textrm{argmin}_{\\textrm{Q} \\in \\mathcal{Q}} D(\\textrm{Q},P_M)\n",
    "$$\n",
    "\n",
    "where $D(\\textrm{Q},P_M)$ is some yet-to-be-defined measure that tells us how similar $\\textrm{Q}$ and $P_M$ are. With this, we can perform inference on $\\textrm{Q}^*$ in place of $P_M$.\n",
    "\n",
    "The first question is: what is our measure of proximity, $D(\\cdot,\\cdot)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A measure between two distributions\n",
    "\n",
    "Recognize how strange this task is. $\\textrm{Q}$ and $P_M$ are *distributions* that map assignments to probabilities. How can we determine whether one approximates another well?\n",
    "\n",
    "Luckily, our information theory pals threw us a lifesaver. That lifesaver is KL-divergence [link], which accepts two probability distributions and returns a non-negative number. If that number is large, then the distributions are very different. If it zero, the distributions are identical. We write KL-divergence as $\\mathbb{KL}(\\textrm{Q}||P_M)$. However, there is one ugly property of KL-divergence - it's not symmetric. In general, $\\mathbb{KL}(\\textrm{Q}||P_M) \\neq \\mathbb{KL}(P_M||\\textrm{Q})$.\n",
    "\n",
    "That said, due to $P_M$'s unweildiness, we are forced to use $\\mathbb{KL}(\\textrm{Q}||P_M)$, which is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(\\textrm{Q}||P_M) = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\log \\frac{\\textrm{Q}(\\mathbf{x})}{P_M(\\mathbf{x})}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we sub in $\\frac{1}{Z}\\tilde{P}_M(\\mathbf{x})$ for $P_M(\\mathbf{x})$, re-arrange things a bit and label what we recognize, we get this neat fella:\n",
    "\n",
    "$$\n",
    "\\mathbb{KL}(\\textrm{Q}||P_M) = \\underbrace{\\big(-\\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\log \\tilde{P}_M(\\mathbf{x})\\big)}_{\\textrm{'Cross Entropy' of }\\textrm{Q and } \\tilde{P}_M,\\space \\mathbb{H}[\\textrm{Q},\\tilde{P}_M]} - \\underbrace{\\big(-\\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} \\textrm{Q}(\\mathbf{x}) \\log \\textrm{Q}(\\mathbf{x})\\big)}_{\\textrm{Entropy of Q},\\space \\mathbb{H}[\\textrm{Q}]} +\\log Z\n",
    "$$\n",
    "\n",
    "This new notation is to guide how we should think about the components of KL-divergence:\n",
    "\n",
    "1. $\\mathbb{H}[\\textrm{Q},\\tilde{P}_M]$: This is called the Cross Entropy of $\\textrm{Q}$ and $\\tilde{P}_M$. It's a measure of *dis*agreement. When this number is high, that means $\\textrm{Q}$ puts high probability on $\\mathbf{x}$'s that $\\tilde{P}_M$ believes are rare.[2]\n",
    "2. $\\mathbb{H}[\\textrm{Q}]$: The entropy of $\\textrm{Q}$ - a number that measures uncertainty. High entropy implies $\\textrm{Q}$ has its probability spread out very thinly and evenly. Low entropy implies it has concentrated probabilities on a small number of $\\mathbf{x}$'s. Sidebar: if you're curious, I wrote more about this here [link].\n",
    "3. $\\log Z$: This is the log normalizer of $P_M$. The choice of $\\textrm{Q}$ can't impact this, so in the context of the optimization, we ignore it.\n",
    "\n",
    "So when we look at:\n",
    "\n",
    "$$\n",
    "\\mathbb{KL}(\\textrm{Q}||P_M) = \\mathbb{H}[\\textrm{Q},\\tilde{P}_M] - \\mathbb{H}[\\textrm{Q}] +\\log Z\n",
    "$$\n",
    "\n",
    "We should think: \"OK, a $\\textrm{Q}$ that does a good job (minimizes $\\mathbb{KL}(\\textrm{Q}||P_M)$) will put high probabilities on $\\mathbf{x}$'s that $\\tilde{P}_M$ puts high weight on (low $\\mathbb{H}[\\textrm{Q},\\tilde{P}_M]$) *and* it'll have it's probabilities spread out thinly (high $\\mathbb{H}[\\textrm{Q}]$).\" Inituitively, I think of $\\mathbb{H}[\\textrm{Q}]$ as our way of choosing between two distributions if they had the same entropy - you want the more uncertain one.\n",
    "\n",
    "There are two major reasons this equation is awesome. \n",
    "\n",
    "The first is that we *don't* need to calculate $P_M(\\mathbf{x})$ ever, which is intractable due to $Z$. We can optimize to the unnormalized version and, theoretically, we're assured to get the same result as though we *did* calculate $P_M(\\mathbf{x})$.\n",
    "\n",
    "The second helps us with that intractable $Z$. Let's say we pull off a perfect optimization and found a $\\textrm{Q}^*$ that obtained a KL-divergence of zero[3]. Well, that would imply:\n",
    "\n",
    "$$\n",
    "\\mathbb{H}[\\textrm{Q}^*] - \\mathbb{H}[\\textrm{Q}^*,\\tilde{P}_M] = \\log Z\n",
    "$$\n",
    "\n",
    "Since we obtained $\\textrm{Q}^*$ by optimizing the terms of the left hand side, we know their values and hence, we now know $Z$. This is huge!\n",
    "\n",
    "Now, we won't actually get a $\\textrm{Q}^*$ that obtains a KL-divergence of 0. But, the better we optimize, the closer this quantity will approach $\\log Z$. So it's an *approximation*:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{H}[\\textrm{Q}] - \\mathbb{H}[\\textrm{Q},\\tilde{P}_M] & \\rightarrow \\log Z \\\\\n",
    "\\textrm{ as }\\mathbb{KL}(\\textrm{Q}||P_M) & \\rightarrow 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It's also a lower bound, so $\\mathbb{H}[\\textrm{Q}] - \\mathbb{H}[\\textrm{Q},\\tilde{P}_M] \\leq \\log Z$ the whole time.\n",
    "\n",
    "This is useful because as we are doing inference, we get an approximation to $Z$ as a bonus. This quantity is typically the hardest part of inference and can be very useful outside of inference (for model selection infact).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mean Field method\n",
    "\n",
    "We still have two more questions to answer: what is our space, $\\mathcal{Q}$, and how do we search it? Resolving these will land us on a specific VI algorithm. Our answers to these will land us on the Mean Field method.\n",
    "\n",
    "So, what is our space, $\\mathcal{Q}$? How about: all distributions that can be expressed as products of univariate distributions. 'Univariate' means each accept an assignment of *one* variable to return a probability. So if there are $w$ variables in $\\mathbf{X}$, then we could expression this as:\n",
    "\n",
    "$$\n",
    "\\{ \\textrm{Q} \\in \\mathcal{Q} \\textrm{ if Q}=\\prod_{i=1}^w q_i(X_i)\\textrm{ for some univariate }q_i(\\cdot)'s\\}\n",
    "$$\n",
    "\n",
    "Now, how do we search this space? Well our choice of $D(\\cdot,\\cdot)$ and $\\mathcal{Q}$ leads us to a particular simple search strategy. Let's write out our optimization problem with these specifics filled in. Since a $\\textrm{Q}$ is defined by a set of $q_i$'s and we are using KL-divergence, our task is to find:\n",
    "\n",
    "$$\n",
    "\\textrm{argmin}_{\\{q_i\\}'s} \\mathbb{KL}\\big[\\prod_i q_i\\big|\\big|P_M\\big]\n",
    "$$\n",
    "\n",
    "To do this, we have an algorithm which updates each $q_i$ iteratively. This falls out of using the Gibbs Rule for $P_M$ and minimizing the KL-divergence with respect to one $q_j$ given all the *other* $q_i$'s. \n",
    "\n",
    "The algorithm starts by initializing all $q_i$'s to some distribution (like the uniform). Then, we pick one (say $q_j$) to update by setting it to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_j(X_j) & = \\frac{1}{Z_j}\\exp\\big(\\sum_{i:X_j \\in \\mathbf{D}_i} \\mathbb{E}_{\\mathbf{D}_i - \\{X_j\\} \\sim \\textrm{Q}}\\big[\\log \\phi_i(\\mathbf{D}_i)\\big]\\big) \\\\\n",
    "& = \\frac{1}{Z_j}\\exp\\big(\\sum_{i:X_j \\in \\mathbf{D}_i} \\sum_{\\mathbf{D}_i}\\log \\phi_i(\\mathbf{D}_i)\\big[\\prod_{X_k \\in \\mathbf{D}_i - \\{X_j\\}} q_k(X_k)\\big] \\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(It's not crucial you understand the details of this equation. If you'd like to, checking out the notation guide from the first answer may help.)\n",
    "\n",
    "The derivation of these updates is a bit complicated[4], but we can still make some useful observations:\n",
    "\n",
    "1. It's remarkable that this update reduces KL-divergence to the whole of $P_M$ but we need only consider factors for which $X_j$ is involved (that's what the '$i:X_j \\in \\mathbf{D}_i$' is specifying). We owe the tractability of this approach to this fact.\n",
    "2. Since $X_j$ is a single variable, the function is sufficiently small that we can calculate the unnormalized probability for each assignment and then sum them up to get the normalizer $Z_j$.\n",
    "\n",
    "After we run through these algorithms, we have a set of distributions, $\\{q_i^*\\}$, that defined our approximation, $\\textrm{Q}^*$, to $P_M$. This factored form makes inference very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see it.\n",
    "\n",
    "Ok, we'll do so with dirt simple example. Let's pretend our 'intractable' $P_M$ is actually a multivariate normal with 2 RVs ($X_1$ and $X_2$) and some diagonal correlation. Picture it like this[5]:\n",
    "\n",
    "![Title](VI_PM.png)\n",
    "\n",
    "Now, let's imagine we initialize $q_1(A)$ and $q_2(B)$ as standard normals. Let's overlay their product (the $\\textrm{Q}$ distribution) onto this graph and the run the Mean Field algorithm, iterating between updating $q_1$ and $q_2$:\n",
    "\n",
    "![Title](VI_iterations.png)\n",
    "\n",
    "As you can observe, the end result is our best choice of $q_1$ and $q_2$ to approximate $P_M$.\n",
    "\n",
    "The first thing to notice is that each update only impacts $\\textrm{Q}$ along one axis. This makes it a kind-of coordinate descent algorithm. These can be slow to move in diagonal directions. \n",
    "\n",
    "Also, you might notice that our approximation doesn't allow for any *joint* behavior - no correlation exists in the approximation. This is a major weakness of the Mean Field method. Fortunately, we could choose our space differently such that this isn't a constraint. Unsurprisngly, many VI algorithms do so. Further, notice that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Broader View\n",
    "\n",
    "From here, we should carefully generalize the weaknesses of the Mean Field algorithm to the broader VI world. Will all VI algorithms be unable to express joint behavior? No, that fell out from our choice of $\\mathcal{Q}$. But, each will suffer whatever inexpressability is created by the constraint of the chosen constrained space. Will all VI algorithms slowly crawl to target distributions that are in diagonal directions? No, but we always have to do a search - that comes with local min/maxs and convergence issues.\n",
    "\n",
    "In general, it's helpful to think about those three questions of VI. What measures agreement between distributions? What is our constrained space of tractable distributions? How do we search it? This fully represents VI in all its creative potential and necessary choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "By this point, you might have had your fill of inference. Maybe you're curious as to how we learn parameters. I'll have two answers coming out on that:\n",
    "\n",
    "[5] How are the parameters of a Bayesian Network learned? [link] (Posting on DATE)\n",
    "\n",
    "[6] How are the parameters of a Markov Network learned? [link] (Posting on DATE)\n",
    "\n",
    "Or, maybe, you want to know about the most general form of approximation inference:\n",
    "\n",
    "[4] How are Monte Carlo methods used to perform inference in Probabilistic Graphical Models? [link] (Posting on DATE)\n",
    "\n",
    "Or, you could find approximate inference complicated and unintuitive. If you haven't already, I'd check out:\n",
    "\n",
    "[2] What is 'exact inference' in the context of Probabilistic Graphical Models? How is it performed? [link] (Posting on DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[1] This is only the case when $\\mathbf{E}$ and $\\mathbf{Y}$ make up the whole of $\\mathcal{X}$.\n",
    "\n",
    "[2] Well technically, it's not a legal Cross Entropy. Cross Entropy is defined over distributions. In this case, it's used for an unnormalized distribution $\\tilde{P}_M$.\n",
    "\n",
    "[3] This would imply $\\textrm{Q}$ *is* $P_M$ in fact.\n",
    "\n",
    "[4] See XYZ section of MLPP for the derivation.\n",
    "\n",
    "[5] Here, you might say: \"Hey, you said everything was discrete!\" Yes, but I don't know how to show discrete visuals well. So pretend this a fine-grained discrete distribution.\n",
    "\n",
    "### Sources\n",
    "\n",
    "[1] Murphy, Kevin. Machine Learning: A Probabilistic Perspective. The MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(Q||P_\\Phi) & = \\mathbb{E}_{\\mathbf{x} \\sim Q}\\big[\\log \\frac{Q(\\mathbf{x})}{P_\\Phi(\\mathbf{x})}\\big]\\\\\n",
    "& = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log \\frac{Q(\\mathbf{x})}{P_\\Phi(\\mathbf{x})}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(Q||P_\\Phi) & = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log \\frac{Q(\\mathbf{x})}{\\frac{1}{Z}\\tilde{P}_\\Phi(\\mathbf{x})}\\\\\n",
    "& = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log Q(\\mathbf{x}) - \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log \\frac{1}{Z}\\tilde{P}_\\Phi(\\mathbf{x}) \\\\\n",
    "& = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log Q(\\mathbf{x}) + \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log Z - \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log \\tilde{P}_\\Phi(\\mathbf{x}) \\\\\n",
    "& = \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log Q(\\mathbf{x}) + \\log Z - \\sum_{\\mathbf{x} \\in Val(\\mathbf{X})} Q(\\mathbf{x}) \\log \\tilde{P}_\\Phi(\\mathbf{x}) \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we can use the following algorithm. Initialize all $q_i(\\cdot)$'s to the uniform distribution. \n",
    "Then, pick one (say $q_j(\\cdot)$) to update by setting it to:\n",
    "\n",
    "$$\n",
    "q_j(X_j) = \\exp\\big(\\sum_{\\mathbf{X} \\setminus X_j}\\prod_{i:i\\neq j} q_i(X_i) \\log \\tilde{P}_\\Phi(\\mathbf{X}) \\big)\n",
    "$$\n",
    "\n",
    "(It's implied that the assignment of $X_j$ is plugged into $\\mathbf{X}$)\n",
    "\n",
    "I'm not deriving it, but take it as truth that with each update, we decrease the KL-divergence[4]. We repeatedly iterative over $q_i()'s$ until they stop changing. In the end, we have a set of $q_i(\\cdot)$ which together give us our $\\textrm{Q}^*$.\n",
    "\n",
    "[Explain why that update equation is remarkable]\n",
    "\n",
    "[DO WE NEED THIS EXPLANATION? I'M PROBABLY DOING IT BECAUSE I DIDN\"T UNDERSTAND HOW IT WORKS!]\n",
    "\n",
    "Some of you might look at that update and say: \"Hey! We can't sum over $\\mathbf{X}$ - there are too many assignments!\" Yes, you're correct. It's time for us to use the Gibbs Rule. That tells us:\n",
    "\n",
    "$$\n",
    "\\tilde{P}_\\Phi(X_1,\\cdots,X_n) = \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i)\n",
    "$$\n",
    "\n",
    "Now, let's split this product into two products: one of which is the product of all factors which involve $X_j$ (call it $\\psi_{X_j}(\\mathbf{C}_{X_j})$) and the other is a product of everything else (call it $\\psi_{\\overline{X_j}}(\\mathbf{C}_{\\overline{X_j}})$). This is just some factor organization that gives us:\n",
    "\n",
    "$$\n",
    "\\tilde{P}_\\Phi(X_1,\\cdots,X_n) = \\psi_{X_j}(\\mathbf{C}_{X_j}) \\psi_{\\overline{X_j}}(\\mathbf{C}_{\\overline{X_j}})\n",
    "$$\n",
    "\n",
    "If we plug this into our update equation, we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_j(X_j) & = \\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\big(\\log \\psi_{X_j}(\\mathbf{C}_{X_j}) + \\log \\psi_{\\overline{X_j}}(\\mathbf{C}_{\\overline{X_j}})\\big)\\prod_{i:i\\neq j} q_i(X_i)  \\bigg)\\\\\n",
    "& = \\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\prod_{i:i\\neq j} q_i(X_i) + \\underbrace{\\sum_{\\mathbf{X} \\setminus X_j}\\log \\psi_{\\overline{X_j}}(\\mathbf{C}_{\\overline{X_j}})\\prod_{i:i\\neq j}q_i(X_i) }_{\\text{Doesn't involve }X_j \\text{ - it's a constant!}}\\bigg)\\\\\n",
    "& = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\prod_{i:i\\neq j} q_i(X_i) \\bigg)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $q_j(X_j)$ gets multiplied by the same constant, regardless of the input, and we know these $q_j(\\cdot)$ is a normalized distribution, then *this constant doesn't matter*. We can ignore it! Ahh!\n",
    "\n",
    "But we are still summing over something nearly as big as $\\mathbf{X}$, but we can make progress:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_j(X_j) & = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\prod_{i:i\\neq j} q_i(X_i) \\bigg)\\\\\n",
    "& = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus \\mathbf{C}_{X_j}} \\sum_{\\mathbf{C}_{X_j}}\\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\big[\\prod_{X_k \\in \\mathbf{X} \\setminus \\mathbf{C}_{X_j}}q_k(X_k)\\big]\\big[\\prod_{X_i \\in \\mathbf{C}_{X_j}} q_i(X_i)\\big] \\bigg)\\\\\n",
    "& = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus \\mathbf{C}_{X_j}}\\big[\\prod_{X_k \\in \\mathbf{X} \\setminus \\mathbf{C}_{X_j}}q_k(X_k)\\big] \\sum_{\\mathbf{C}_{X_j}}\\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\big[\\prod_{X_i \\in \\mathbf{C}_{X_j}} q_i(X_i)\\big] \\bigg)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_j(X_j) & = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\prod_{i:i\\neq j} q_i(X_i) \\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\bigg)\\\\\n",
    "& = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus X_j}\\big[\\prod_{X_k \\in \\mathbf{X} \\setminus \\mathbf{C}_{X_j}}q_k(X_k)\\big]\\big[\\prod_{X_i \\in \\mathbf{C}_{X_j}} q_i(X_i) \\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\big]\\bigg)\\\\\n",
    "& = \\text{const}\\cdot\\exp\\bigg(\\sum_{\\mathbf{X} \\setminus \\mathbf{C}_{X_j}} \\sum_{\\mathbf{C}_{X_j}}\\big[\\prod_{X_k \\in \\mathbf{X} \\setminus \\mathbf{C}_{X_j}}q_k(X_k)\\big]\\big[\\prod_{X_i \\in \\mathbf{C}_{X_j}} q_i(X_i) \\log \\psi_{X_j}(\\mathbf{C}_{X_j})\\big]\\bigg)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
