{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Chapter 11\n",
    "\n",
    "- we approximate the target distribution with a simpler class of distribution, for which inference is easier.\n",
    "- This task makes it a constrained optimization problem. Lagrange multipliers are frequently used to do so. They provide a set of equations that characterize the solution.\n",
    "- An awesome result: the 'fixed-point' equations derived from the constrained optimization, for any of the methods we describe, can be viewed as passing message over a graph object.\n",
    "- methods fall into 3 categories:\n",
    "    - clique-tree message passing on structures that aren't tree (where it's guaranteed to be exact). So this is LBP.\n",
    "    - clique-tree message passing with approximate messages\n",
    "    - those that generalize the mean-field message, but are only done on a certain class of Q (the simplier distributions).\n",
    "- each method can be described from two perspectives:\n",
    "    - the procedural perspective\n",
    "    - an optimization over a constrained space. This perspective shows that message passing is just one way to perform the optimization.\n",
    "- viewing exact inference as an optimization: A cluster tree represents a set of distributions Q.. exact inference is a *search* to find Q in there that matches $P_\\Phi$\n",
    "- We need methods that will optimize the distance between a Q and P, but allow us to still perform infernce on Q. So we avoid that hard questions on P.\n",
    "- KL divergence, D(P|Q), actually requires inference on P. So don't do it! We can get away with D(Q|P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer structure\"\n",
    "\n",
    "1. Intro\n",
    "2. Refresher\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is 'approximate inference' in the context of Probabilistic Graphical Models? How is it done?\n",
    "\n",
    "(This is the 3rd answer in a 4 part series on Probabilistic Graphical Models).\n",
    "\n",
    "Inference, as mentioned last time, is the task of using a model of a system to calculate answers regarding that system. In my previous answer, I showed how we would do this *exactly*. Now, we relax that requirement - we demand our algorithms merely calculate these answers *approximately*.\n",
    "\n",
    "The destruction of the exact-constraint is liberating - approximate inference enjoys a much wider space of candidate algorithms. The difficulty, however, is that it's harder to sow a single thread through all of these. At best, we can divide them into two groups:\n",
    "\n",
    "1. Variational Methods\n",
    "2. Monte Carlo Methods\n",
    "\n",
    "So I\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A refresher (same refresher from the 2nd answer)\n",
    "\n",
    "In the first answer [link], we discovered why Probabilistic Graphical Models (PGMs) are useful tools for studying complex system. To recap, a complex system is a set of random variables (which we'll call $\\mathcal{X}$) with a relationship we'd like to understand. We are given some, at least partial, joint observations of $\\mathcal{X}$. We've decided that a good understanding of our system means we can answer:\n",
    "\n",
    "1. **Probability Queries**: Compute the probabilities $P(\\mathbf{Y}|\\mathbf{X}=\\mathbf{x})$. What is the distribution of the RV's of $\\mathbf{Y}$ given we have some observation of those RVs of $\\mathbf{X}$?\n",
    "2. **MAP Queries**: Determine $\\textrm{argmax}_\\mathbf{y}P(\\mathbf{Y}=\\mathbf{y},\\mathbf{X}=\\mathbf{x})$ when given $\\mathbf{x}$. That is, determine the most likely values of random variables (RVs) associated with an assignment of other RVs.\n",
    "\n",
    "(If this notation is unfamiliar, see the 'Preamble Notes' section from the first answer).\n",
    "\n",
    "We discovered that even if we had the joint distribution of $\\mathcal{X}$ (called $P$), we could only answer these questions if we had certain simplifications on $P$. Those simplifications were **conditional independence (CI) statements**. If such statements are true for $P$ (stated as '$P$ satisfies these CI statements'), it simplifies calculations on $P$.\n",
    "\n",
    "The big innovation of PGMs is that we may *represent* sets of CI statement with a graph of nodes and edges, and therefore, represent all $P$'s which satisfy them. The two dominate types of PGMs are the Bayesian Network (BN) and the Markov Network (MN). A BN is associated with a graph with *directed* edges (a DAG) and uses the Chain Rule, along with Conditional Probability Tables ('CPTs' or 'CPDs'), to determine probabilities. A choice of CPDs makes the BN recreate the probabilities of a specific $P$. Similarly, a MN is associated with an *undirected* graph, which may represent a different class of CI statements. Instead of the Chain Rule, a MN uses functions called *factors* to calculate probabilities. I called this the 'Gibbs Rule'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
