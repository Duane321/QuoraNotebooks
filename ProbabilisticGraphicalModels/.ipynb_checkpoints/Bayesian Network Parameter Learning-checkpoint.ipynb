{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Section 17.2 - MLE for Bayesian Networks\n",
    "\n",
    "Ideas to mention\n",
    "- The likelihood function - Parameters impact the chain rule, that tells us how to calc probabilities. We maximize those probabilities for our data\n",
    "- The decomposition of the likelihood\n",
    "- Bayesian parameter estimation.\n",
    "- Prediction? Integrating the parameters out.\n",
    "- What happens with missing data?\n",
    "\n",
    "Answer structure:\n",
    "\n",
    "- Intro\n",
    "    -  \n",
    "- Refresher\n",
    "    - \n",
    "- Where are we starting and what's the goal?\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are the parameters of a Bayesian Network learned?\n",
    "\n",
    "(This is the 5th answer in a 7 part series [link] on Probabilistic Graphical Models ('PGMs'))\n",
    "\n",
    "Up until now, our starting point is that we've been given a PGM (of which a Bayesian Network (a 'BN') is an certain type), complete with fitted parameters. Here, we'll discover how to determine (i.e. to 'learn') those parameters from data. I'll say up front, this task for a BN is as friendly as parameter learning gets - that's why its our first destination. In doing so, we'll take a tour of the major considerations of parameter learning and their relatively simple solutions for a BN. When we're faced with more complicated parameter learning tasks, the approach here will serve as a useful framework to contextualize those difficulties.\n",
    "\n",
    "As promised, let's recap some relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    "\n",
    "In the first answer [link], we discovered why PGMs are useful tools for representing complex system. We defined a complex system as a set of $n$ random variables (which we call $\\mathcal{X}$) with a relationship we'd like to understand. We take it that there exists some true but unknown joint distribution, $P$, which govern these random variables ('RVs'). We take it that a 'good understanding' means we can answer two types of questions regarding this $P$:\n",
    "\n",
    "1. **Probability Queries**: Compute the probabilities $P(\\mathbf{Y}|\\mathbf{e})$. What is the distribution of the RV's of $\\mathbf{Y}$ given we have some observation ($\\mathbf{e}$) of the RVs of $\\mathbf{E}$?\n",
    "2. **MAP Queries**: Determine $\\textrm{argmax}_\\mathbf{Y}P(\\mathbf{Y}|\\mathbf{e})$. That is, determine the most likely values of some RVs given an assignment of other RVs.\n",
    "\n",
    "(Where $\\mathbf{E}$ and $\\mathbf{Y}$ are two arbitrary subsets of $\\mathcal{X}$. If this notation is unfamiliar, see the 'Notation Guide' section from the first answer [link]).\n",
    "\n",
    "The idea behind PGMs is to estimate $P$ using two things:\n",
    "\n",
    "1. A graph: a set of nodes, each of which represents an RV from $\\mathcal{X}$, and a set of edges between these nodes.\n",
    "2. Parameters: objects that, when paired with a graph and a certain rule, allow us to calculate probabilities of assignments of $\\mathcal{X}$.\n",
    "\n",
    "One of the two main categories of PGM is the Bayesian Networks. A **Bayesian Network** involves a graph, denoted as $\\mathcal{G}$, with *directed* edges and no directed cycles. The parameters are Conditional Probability Tables ('CPDs' or 'CPTs'), which are, as the naming suggests, select conditional probabilities from the BN. They give us the right hand side of the Chain Rule, which dictates we calculate probabilities according to a BN:\n",
    "\n",
    "$$\n",
    "P_{B}(X_1,\\cdots,X_n)=\\prod_{i=1}^n P_{B}(X_i|\\textrm{Pa}_{X_i}^\\mathcal{G})\n",
    "$$\n",
    "\n",
    "where $\\textrm{Pa}_{X_i}^\\mathcal{G}$ indicates the set of parents nodes/RVs of $X_i$ according to $\\mathcal{G}$.\n",
    "\n",
    "To illustrate, we can consider a well utilized example from the text Probabilistic Graphical Models (link). It's called the 'Student Bayesian Network'. Here, we're concerned with a system of five RVs: a student's intelligence ($I$), their class's difficulty ($D$), their grade in that class ($G$), their letter of recommendation ($L$) and their SAT score ($S$). So $\\mathcal{X}=\\{I,D,G,L,S\\}$. The BN graph along with the CPDs can be represented as:\n",
    "\n",
    "![title](StudentBN.png)\n",
    "\n",
    "According to our rule, we have that any joint assignment of $\\mathcal{X}$ factors as:\n",
    "\n",
    "$$\n",
    "P_B(I,D,G,S,L) = P_B(I)P_B(D)P_B(G|I,D)P_B(S|I)P_B(L|G)\n",
    "$$\n",
    "\n",
    "So we would calculate a given assignment as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_B(i^1,d^0,g^2,s^1,l^0) = & P_B(i^1)P_B(d^0)P_B(g^2|i^1,d^0)P_B(s^1|i^1)P_B(l^0|g^2)\\\\\n",
    "= & 0.3\\cdot 0.6\\cdot 0.08\\cdot 0.8\\cdot0.4 \\\\\n",
    "= & 0.004608\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, everything we've discussed so far has assumed those tables are *given* to us. Not anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's our starting point and what's our goal?\n",
    "\n",
    "Our job is to determine the CPDs associated with the *given graph* (so think nodes and edges) of a BN. We are assisted by some, at least partial, observations of $\\mathcal{X}$. To start, we'll assume our observations of $\\mathcal{X}$ are *complete* - each piece of data is an observation of *all* RVs of $\\mathcal{X}$. Later, we'll relax this assumption and consequently, ruin our lives. \n",
    "\n",
    "But, with our lives pre-ruined, let's introduce some notation. Let's say $\\mathbf{X} = \\mathcal{X}$ and we have a set of $w$ observations that we'll write as $\\mathcal{D}=\\{\\mathbf{x}^{(i)}\\}_{i=1}^w$. So we might have $\\mathbf{x}^{(1)}=[i^0,d^1,g^1,s^0,l^1]$ from our previous example. The fact that the data is 'complete' means no entries are missing.\n",
    "\n",
    "A mental picture will help[0]:\n",
    "\n",
    "![title](xmt.png)\n",
    "\n",
    "Even though there are 5 RVs, this isn't the Student example. Also, this picture comes with no graph structure - it's just a representation of $\\mathcal{D}$.\n",
    "\n",
    "Our goal is to use this block of data to form our tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "To do so, we have to introduce the **likelihood function**. A likelihood function accepts parameters and returns a number that tells us how *likely* a fixed set of data is according to those parameters. The guiding principle is to pick parameters that maximize this function, i.e. the likelihood of our data. Such parameters are called our **Maximum Likelihood Estimate ('MLE').** To write this out, it's typical to say $\\boldsymbol{\\theta}$ is a vector that lists out our parameters that our likelihood function is $\\mathcal{L}(\\boldsymbol{\\theta})$. Further, the argmax of such a function is the same as the argmax of the *log* of this function, and since log-ing turns difficult multiplication into easy addition, we maximize the log likelihood, $\\log \\mathcal{L}(\\boldsymbol{\\theta})$. In the case of a BN, this function accepts a vector of parameters, uses them to form the CPDs, calculates each data point's log probability according to the Chain Rule, and returns the sum of these numbers. We may write this as:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^w \\sum_{j=1}^n \\log P_{B}(X_j=x^{(i)}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(i)}_{\\textrm{Pa}_{X_j}^\\mathcal{G}})\n",
    "$$\n",
    "\n",
    "To make this clear, let's calculate a chunk of this expression, that for which $i=1$, for the Student example: $\\mathbf{x}^{(1)}=[i^1,d^0,g^2,s^1,l^0]$. Further, let's say $\\boldsymbol{\\theta}$ lists out the entries in the tables we used in that example. Then that chunk is[2]:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{j=1}^n \\log P_{B}(X_j=x^{(1)}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(1)}_{\\textrm{Pa}_{X_j}^\\mathcal{G}})&=\\log P_B(i^1)+\\log P_B(d^0)  \\\\\n",
    "& + \\log P_B(g^2|i^1,d^0) + \\log P_B(s^1|i^1)+\\log P_B(l^0|g^2)\\\\\n",
    "& = \\log 0.3+ \\log 0.6+ \\log 0.08+ \\log 0.8+ \\log 0.4\\\\\n",
    "& \\approx -5.38 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Pictorially, we just calculated the chunk of $\\log \\mathcal{L}(\\boldsymbol{\\theta})$ that comes from the first row of $\\mathcal{D}$:\n",
    "\n",
    "![title](xmt_firstrow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing the likelihood function with complete data\n",
    "\n",
    "It turns out, fortunately, that if we have complete data, it's very easy to calculate $\\boldsymbol{\\theta}^* = \\textrm{argmax} \\log \\mathcal{L}(\\boldsymbol{\\theta})$. To see this, let's flip the order of the summation signs and label:\n",
    "\n",
    "(Use picture editing to condense this into one line)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\theta}) = & \\overbrace{\\sum_{i=1}^w \\sum_{j=1}^n}^{\\textrm{Switched these!}} \\log P_{B}(X_j=x^{(i)}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(i)}_{\\textrm{Pa}_{X_j}^\\mathcal{G}}) \\\\\n",
    " = &  \\sum_{j=1}^n \\underbrace{\\sum_{i=1}^w \\log P_{B}(X_j=x^{(i)}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(i)}_{\\textrm{Pa}_{X_j}^\\mathcal{G}})}_{\\textrm{Call this } \\log \\mathcal{L}_j(\\boldsymbol{\\theta})} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we can think of this as a sum of $n$ terms: $\\sum_{j=1}^n \\log \\mathcal{L}_j(\\boldsymbol{\\theta})$. Here, for example, is how to think about $\\mathcal{L}_3(\\boldsymbol{\\theta})$ (where, in this example, $\\textrm{Pa}_{X_j}^\\mathcal{G}=\\{X_4,X_5\\}$):\n",
    "\n",
    "![title](xmt_X3.png)\n",
    "\n",
    "There is one such chunk for each RV. You can imagine how many fewer such chunks there are then rows.\n",
    "\n",
    "But beyond this save on compute, there is a very important idea lurking nearby:\n",
    "\n",
    "\"\n",
    "$\\log \\mathcal{L}_j(\\boldsymbol{\\theta})$ depends *only* on a subset of elements from $\\boldsymbol{\\theta}$ - those that tell us the elements of the $ P_{B}(X_j|\\textrm{Pa}_{X_j}^\\mathcal{G})$ CPD (let's call them $\\boldsymbol{\\theta}_{X_j|\\textrm{Pa}_{X_j}^\\mathcal{G}}$)\n",
    "\"\n",
    "\n",
    "This means  we can optimize $\\log \\mathcal{L}_j(\\boldsymbol{\\theta})$ *independently*, determining the best $\\boldsymbol{\\theta}_{X_j|\\textrm{Pa}_{X_j}^\\mathcal{G}}^*$ for each and then pasting them together to get $\\boldsymbol{\\theta}^*$. The best choice of $\\boldsymbol{\\theta}_{X_j|\\textrm{Pa}_{X_j}^\\mathcal{G}}$ will never depend on the best choice of $\\boldsymbol{\\theta}_{X_i|\\textrm{Pa}_{X_i}^\\mathcal{G}}$ when $j\\neq i$. This is a major triump over the curve of dimensionality!\n",
    "\n",
    "Now, let's say $\\boldsymbol{\\theta}$ tells us our CPDs by simply listing out their elements. In this case, it's even easier. We don't need to run an optimization for each $\\log \\mathcal{L}_j(\\boldsymbol{\\theta})$ - we can calculate the answer directly from the data. Specifically, the answer is the $\\boldsymbol{\\theta}_{X_j|\\textrm{Pa}_{X_j}^\\mathcal{G}}^*$ such the corresponding CPD is the *empirical* conditional distribution. Easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "(Need to add something about the observation process)\n",
    "\n",
    "If we have missing data, we have to adjust $\\mathcal{L}(\\boldsymbol{\\theta})$ to account for all possible ways our hidden variables *could be*. So, let's say we come across:\n",
    "\n",
    "![title](xmt_missing.png)\n",
    "\n",
    "Then, for each row, we have to consider all possible *joint* assignments of the hidden variables. To see this, let $h(i)$ be the number of possible joint assignments to the hidden variables at row $i$. So, if $Val(X_1)$ has 3 elements and $Val(X_4)$ has 4 elements, then $h(2)=3 \\times 4 = 12$. Let's also say $k$ indexes over these assignments of hidden variables and $\\mathbf{x}^{(i),k}$ is $\\mathbf{x}^{(i)}$ but with the $k$-th hidden variable assignment plugged in. In this case, our likelihood function becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = \\prod_{i=1}^w \\sum_{k=1}^{h(i)}\\prod_{j=1}^n P_{B}(X_j=x^{(i),k}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(i),k}_{\\textrm{Pa}_{X_j}^\\mathcal{G}})\n",
    "$$\n",
    "\n",
    "That summation sign is devastating. Look what we get for the log likelihood:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^w \\log \\big(\\sum_{k=1}^{h(i)}\\prod_{j=1}^n P_{B}(X_j=x^{(i),k}_j|\\textrm{Pa}_{X_j}^\\mathcal{G}=\\mathbf{x}^{(i),k}_{\\textrm{Pa}_{X_j}^\\mathcal{G}})\\big)\n",
    "$$\n",
    "\n",
    "No! A log of a sum of products! We can't simplify that! In this form, we have to proceed row by row. You can picture this ugliness like this:\n",
    "\n",
    "![title](xmt_missing_summed.png)\n",
    "\n",
    "The summation is over all possible paths for each row. These path may traverse those large column blocks we had earlier, so we can no longer separate this problem into those blocks. Further, you can see visually how this adds a dimension to our problem.\n",
    "\n",
    "Though, all is not lost. Missing data complicates our objective function, but we may still optimize it. To help us, we can still calculate a gradient and throw this into a generic optimizer. We can also use the very handy EM Algorithm [link]. Or, if we have a small portion of hidden elements, we can simply drop the associated row or column and proceed as though it's complete date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about regularization?\n",
    "\n",
    "Yes, good point. Earlier when I mentioned that *if* our parameters list out the elements of the CPDs and we have complete data, then the MLE is just the empirical conditional distributions. What that means is we'd need to observe *every* combination of parent assignments to have such an answer. This never happens in practice. Even if $\\boldsymbol{\\theta}$ isn't specified that way, issues can arise for similar reasons.\n",
    "\n",
    "Fortunately, this is a well addressed problem. The solution is... to be Bayesian! We treat $\\boldsymbol{\\theta}$ like an RV and add it to the graph with it's own, exogenously decided, CPD. The effect is to bias the parameters towards what that CPD favors in the absence of sufficient data. In such a case, $\\boldsymbol{\\theta}$ is an RV that we *never* observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is that it?\n",
    "\n",
    "Yep, pretty much. Though I'm sweeping substantial practical complexity under the rug. How you choose your parameterization of the BN, i.e. how $\\boldsymbol{\\theta}$ exactly relates to the CPDs, is a *huge* consideration. Maybe it does simply list out the elements in the CPDs, or maybe, each CPD is a 100-layer neural network with thousands of parameters. The realized complexity of the implementation depends heavily on these choices.\n",
    "\n",
    "But beyond that, this is a surprisingly full view. Given a graph of nodes/edges, a choice of parameterization and some data, we can form an objective to optimize. The input that does so gives us our CPDs! Woo hoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[0] This picture assumes 5 RVs, but that's just to avoid more ugly dots. Pretend it's $n$ RVs.\n",
    "\n",
    "[1] I'm saying 'to parameterize' to keep things general. It may be the case that the elements of $\\boldsymbol{\\theta}$ list out the elements of our CPDs. But, we could have other parameterizations, as long as each CPD is attested to by its own set of elemenents from $\\boldsymbol{\\theta}$.\n",
    "\n",
    "[2] I'm skirting a detail - we don't necessary sum up in the order $1, 2, \\cdots, n$. We need to order things such that when we come across a $P_{B}(X_i|\\textrm{Pa}_{X_i}^\\mathcal{G})$, we've already calculated a similar conditional probability for all RVs in $\\textrm{Pa}_{X_i}^\\mathcal{G}$. This is always possible in a BN Graph and such an ordering is called the *topological ordering*. There may be more than one such ordering and it doesn't change the output of the Chain Rule.\n",
    "\n",
    "[3] This is your classic $\\mathbf{X}$ matrix you're probably familiar with. I'm not calling it that because that notation is already occupied.\n",
    "\n",
    "[4] This assumes we have $m=?$ observations and $n=?$ random variables, obviously.\n",
    "\n",
    "### Sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
