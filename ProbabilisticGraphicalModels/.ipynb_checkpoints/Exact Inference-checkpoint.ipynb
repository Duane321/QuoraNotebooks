{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Chapter 9 - Exact inference\n",
    "- CI statements and factorization of the joint are critical for performing inference\n",
    "- Focus of the chapter: the conditional probability query: P(Y|E=e)\n",
    "- Then it discusses how we would sum over the joint $P$ to calc this query.\n",
    "- The problem of both exact and approximate inference (approximate with a relative error metric) even in a graphical model is NP-Hard in the *worse case*. In practice, we can make substantial progress. \n",
    "- 'Because any Bayesian network can be encoded as a Markov network with no increase in its representation size, a hardness proof for inference in Bayesian networks immediately implies hardness of inference in Markov networks.'\n",
    "- Once we have a BN, we don't want to use it to calc all joint probs. This defeats the purpose.\n",
    "- The same graphical structure that allows for a compact representation of complex distributions also helps support exact inference.\n",
    "- Variable Elimination\n",
    "\n",
    "Elsewhere\n",
    "- variable elimnation can also be used to compute MAP estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer outline\n",
    "\n",
    "- Introduce the previous question and recap those topics.\n",
    "- Define what we are doing (which queries?)\n",
    "- There's a problem! This is NP-hard\n",
    "- We can always write a BN as a MN, so let's do that.\n",
    "- Exact inference\n",
    "- Approximate inference\n",
    "- Wrapping it up??\n",
    "\n",
    "What do I need to include?\n",
    "\n",
    "- The definition of inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is 'exact inference' in the context of Probabilistic Graphical Models? How is it performed? \n",
    "\n",
    "(This is the 2nd answer in a 7 part series [link] on Probabilistic Graphical Models ('PGMs'). At the least, I suggest checking out the notation guide from that linked 1st answer.)\n",
    "\n",
    "This is how we'll define inference:\n",
    "\n",
    "\"The task of using a given PGM of a system, complete with fitted parameters, to answer certain questions regarding that system.\"\n",
    "\n",
    "For such questions, an exact answer exists. Exact inference algorithms calculate such answers *exactly.* It's an extremely difficult, though well defined, class of problem. The set of mind-bending algorithms to tackle such an issue would intimidate any aspiring machine learn-er.\n",
    "\n",
    "But one realization relaxed me in the face of these precise beasts: they are all built to exploit **factorization**. You know, the act of turning $ac + ad + bc + bd$ into $(a+b)(c+d)$. Knowing this doesn't make these algorithms simple (they aren't) but it certainly demystifies them.\n",
    "\n",
    "Another frequently exploited principle is **caching calculations for re-use**. The monumental calculous involved often have a fair amount of redundancy - keeping track of past work substantially reduce compute time.\n",
    "\n",
    "But before we can observe these powerful hacks, let's do a short review:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    "\n",
    "In the first answer [link], we discovered why PGMs are useful tools for representing complex system. We defined a complex system as a set of $n$ random variables (which we call $\\mathcal{X}$) with a relationship we'd like to understand. We take it that there exists some true but unknown joint distribution, $P$, which govern these RVs. We take it that a 'good understanding' means we can answer two types of questions regarding this $P$:\n",
    "\n",
    "1. **Probability Queries**: Compute the probabilities $P(\\mathbf{Y}|\\mathbf{e})$. What is the distribution of the RV's of $\\mathbf{Y}$ given we have some observation ($\\mathbf{e}$) of the RVs of $\\mathbf{E}$?\n",
    "2. **MAP Queries**: Determine $\\textrm{argmax}_\\mathbf{Y}P(\\mathbf{Y}|\\mathbf{e})$. That is, determine the most likely values of some RVs given an assignment of other RVs.\n",
    "\n",
    "(Where $\\mathbf{E}$ and $\\mathbf{Y}$ are two arbitrary subsets of $\\mathcal{X}$. If this notation is unfamiliar, see the 'Notation Guide' section from the first answer [link]).\n",
    "\n",
    "The idea behind PGMs is to estimate $P$ using two things:\n",
    "\n",
    "1. A graph: a set of nodes, each of which represents an RV from $\\mathcal{X}$, and a set of edges between these nodes.\n",
    "2. Parameters: objects that, when paired with a graph and a certain rule, allow us to calculate probabilities of assignments of $\\mathcal{X}$.\n",
    "\n",
    "PGMs fall into two main categories, Bayesian Networks ('BNs') and Markov Networks ('MNs'), depending on the specifics of these two.\n",
    "\n",
    "A **Bayesian Network** involves a graph, denoted as $\\mathcal{G}$, with *directed* edges and no directed cycles. The parameters are Conditional Probability Tables ('CPDs' or 'CPTs'), which are, as the naming suggests, select conditional probabilities from the BN. They give us the right hand side of the Chain Rule, which dictates we calculate probabilities according to a BN:\n",
    "\n",
    "$$\n",
    "P_{B}(X_1,\\cdots,X_n)=\\prod_{i=1}^n P_{B}(X_i|\\textrm{Pa}_{X_i}^\\mathcal{G})\n",
    "$$\n",
    "\n",
    "where $\\textrm{Pa}_{X_i}^\\mathcal{G}$ indicates the set of parents nodes/RVs of $X_i$ according to $\\mathcal{G}$.\n",
    "\n",
    "A **Markov Network**'s graph, denoted as $\\mathcal{H}$, is different in that it's edges are *undirected* and we may have cycles. The parameters are a size $m$ set of *functions* which map assignments of $m$ subsets of $\\mathcal{X}$ to nonnegative numbers. Those subsets, which we'll call $\\mathbf{D}_i$'s, correspond to *complete subgraphs* of $\\mathcal{H}$ and their union makes up the whole of $\\mathcal{H}$. We can refer to this set as $\\Phi=\\{\\phi_i(\\cdots)\\}_{i=1}^m$. With that, we say that the 'Gibbs Rule' for calculation probabilities is:\n",
    "\n",
    "$$\n",
    "P_M(X_1,\\cdots,X_n) = \\frac{1}{Z} \\underbrace{\\prod_{i = 1}^m \\phi_i(\\mathbf{D}_i)}_{\\text{we call this }\\tilde{P}_M(X_1,\\cdots,X_n)}\n",
    "$$\n",
    "\n",
    "where $Z$ is a normalizer - it ensures our probabilities sum to 1.\n",
    "\n",
    "Lastly, we recall that the Gibbs Rule may expression the Chain Rule. That is, we can always recreate the probabilities produced by a BN's Chain Rule with an another invented MN and its Gibbs Rule. Essentially, we define factors as those that reproduce looking up a conditional probability in a BN's CPDs. This equivalence allows us to reason solely in terms of the Gibbs Rule, while assured that whatever we discover will also hold for BNs. In other words, with regards to inference, if something works for $P_M$, then it works for $P_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whew, ok, so, now... what is happening?\n",
    "\n",
    "To repeat, we are concerned with the task of inference. That task is to calculate those previously mentioned queries, given we have our graph and it's parameters (CPDs for BNs and factors for MNs) determined. There are two types: exact inference and approximate inference, which are exactly what they sound like. The former calculates these quieries *exactly*. The latter does so *approximately*. Here, we'll do things exactly and and answer 3 [link] and 4 [link] will do so approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gibbs Table\n",
    "\n",
    "To understand exact inference, there is one table you should picture the whole time. That table (call it the 'Gibbs table') lists out all possible assignments of $\\mathcal{X}$ and the *unnormarlized* product of the factors associated with that assignment. That unnormalized product is an unnormalized *probability*, so we call it $\\tilde{P}(\\cdots)$.\n",
    "\n",
    "Let's consider an example. Let's say we have the system $\\mathcal{X}=\\{C,D,I,G,S\\}$, where each RV can take one of two values. For example, only the assignments $C=c^0$ or $C=c^1$ may occur for the RV $C$. Let's also say we have these factors determined:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\phi_1(C) \\\\\n",
    "&\\phi_2(C,D) \\\\\n",
    "&\\phi_3(I) \\\\\n",
    "&\\phi_4(G,I,D) \\\\\n",
    "&\\phi_5(S,I) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By 'determined', we mean we know the nonnegative number each of these functions map to for any assignments of their RVs. For example, we might have:\n",
    "\n",
    "![title](FactorOutput.png)\n",
    "\n",
    "Now we can picture the Gibbs table:\n",
    "\n",
    "![title](GibbsTable.png)\n",
    "\n",
    "I'm not showing all of it because it has $2^5=32$ rows. In general, this table is exponentially large, so we want to *think* about it, but never actually write it down. Also, we should point out that our normalizer, $Z$, is the summation of the $\\tilde{P}(C,D,I,G,S)$ column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning is just filtering the Gibbs Table\n",
    "\n",
    "With this view, let's consider our probability query $P_M(\\mathbf{Y}|\\mathbf{e})$. Bayes theorem tells us: $P_M(\\mathbf{Y}|\\mathbf{e})=\\frac{P_M(\\mathbf{Y} , \\mathbf{e})}{P_M(\\mathbf{e})}$. Both the numerator and the denominator concern rows of the Gibbs Table where $\\mathbf{E}=\\mathbf{e}$. Specifically, the denominator is the sum of all such rows and the numerator refers to a partitioning of these rows into groups, where any two rows are in the same group if they have the same assignments to $\\mathbf{Y}$. I'll make this more clear with an example.\n",
    "\n",
    "But first, we should recognize a broader point. The act of conditioning makes all rows that *don't* agree with our observation *irrelevant* - we can throw them away! Said differently, anytime we deal with a factor that involves $\\mathbf{E}$, we plug in the $\\mathbf{e}$-values, so we don't care about the factor outputs for non-$\\mathbf{e}$-values. This means we can think of defining a *new* MN with $\\mathbf{E}=\\mathbf{e}$ baked in. This is called 'reducing' the Markov Network. The result is a new graph (call it $\\mathcal{H}_{|\\mathbf{e}}$) where we delete the $\\mathbf{E}$ nodes and any edges that involve them. Also, we get a new set of factors, (call it $\\Phi_{|\\mathbf{e}}$) which are our original factors, but with the $\\mathbf{e}$-values fixed as inputs. The normalizer in this reduced network, $Z_{|\\mathbf{e}}$, is just $P_M(\\mathbf{e})$.\n",
    "\n",
    "So just think: conditioning means we filter the Gibbs table to where $\\mathbf{E}=\\mathbf{e}$ and those rows can be consider their own MN.\n",
    "\n",
    "But I still owe you an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of naively calculating $P_M(\\mathbf{Y}|\\mathbf{e})$\n",
    "\n",
    "Let's say $\\mathbf{Y}=\\{D,I\\}$ and $\\mathbf{E}=\\{G\\}$, where we are dealing with the observation $G=g^0$. So our probability query is 'What is the distribution of $\\{D,I\\}$ given we observe $G=g^0$?' In other words, we want to calculate $P(D,I|g^0)$.\n",
    "\n",
    "Let's start simple and consider just the probability $P_M(d^0,i^1|g^0)$. Following the previous argument, we start by filtering the table to rows where $G=g^0$. If we sum these rows, we'll get our Bayes Theorem denominator, $P_M(g^0)$. Of these rows, if we sum those where $D=d^0$ and $I=i^1$, we'll get our numerator, $P_M(d^0,i^1, g^0)$ and have all we need to calculate $P_M(d^0,i^1|g^0)$. With a managably sized table, you could do this in Excel in 60 seconds.\n",
    "\n",
    "From this, we can tell that the core process is filtering to rows that agree with some assignment and summing up the unnormalized probabilities. So lets refer to this as a 'row sum' function $rs(\\cdots)$. Specifically, $rs(d^0,i^1, g^0)$ would give us that numerator and $rs(g^0)$ would give the denominator. So we calculated:\n",
    "\n",
    "$$\n",
    "P_M(d^0,i^1|g^0) = \\frac{rs(d^0,i^1, g^0)}{rs(g^0)}\n",
    "$$\n",
    "\n",
    "But, there's an issue. These summations could be over an exponential number of rows, making the summation impossible. We have to be clever and efficient.\n",
    "\n",
    "### The first easy compute saver.\n",
    "\n",
    "Here's an easy way to save some compute. Ultimately, we care about all probabilities for all joint assignments of $D$ and $I$, not just $D=d^0$ and $I=i^1$. In other words, we want to calculate all 4 of these:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_M(d^0,i^0|g^0) & = \\frac{rs(d^0,i^0,g^0)}{rs(g^0)} \\\\\n",
    "P_M(d^0,i^1|g^0) & = \\frac{rs(d^0,i^1,g^0)}{rs(g^0)} \\\\\n",
    "P_M(d^1,i^0|g^0) & = \\frac{rs(d^1,i^0,g^0)}{rs(g^0)} \\\\\n",
    "P_M(d^1,i^1|g^0) & = \\frac{rs(d^1,i^1,g^0)}{rs(g^0)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Well, because an observation of $g^0$ must be associated with one and only one assignment of $D$ and $I$, then those assignments *partition* the rows associated with $g^0$ into 4 groups. So we realize that those numerators sum up to the denominator, which is the same in all cases. That is, we have:\n",
    "\n",
    "$$\n",
    "rs(g^0) = \\sum_D \\sum_I rs(D,I, g^0)\n",
    "$$\n",
    "\n",
    "(On notation: if you see an RV underneath a summation sign, that means we are to 'sum over all assignments of that RV' for the expression that follows. If it's bold, then it's assignment over all *joint* assignments of those RVs. We hide the lowercase assignment notation in these cases.)\n",
    "\n",
    "This means we never need to do the work of calculating $rs(g^0)$ directly. Just calculate the numerators *first*, and add them up at the end to get the denominator.\n",
    "\n",
    "So, on a general note, we just need the ability to calculate this row sum function for any given joint assignment of whatever subset of $\\mathcal{X}$. Then we can answer the probability queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factoring and caching are our best friends.\n",
    "\n",
    "We've reduced the problem to computing $rs(\\cdots)$ effeciently. As an arbitrary example, let's consider the rows of $rs(c^1)$:\n",
    "\n",
    "![title](GibbsTable_filtered.png)\n",
    "\n",
    "Ugh, what a headache! No matter - algorithms are intended for such ugly complexity.\n",
    "\n",
    "To get there, let's count the simple add/multiple calculations involved in calculating this naively. We have 4 multiplications per row and 15 summation, giving us $4 \\times 16 + 15 = 79$ calculations. In the non-toy case, this number would be exponentially frightening. Hmm, can we get it down? Certainly!\n",
    "\n",
    "First of all, why are we multiplying $\\phi_1(c^1)$ for each row, when it's the same thing each time? If we factor that out of the summation, we'd save 15 calculations. \n",
    "\n",
    "Ok, what else can we do? Let's consider summing the first 2 rows (with $\\phi_1(c^1)$ pulled out). That's this guy:\n",
    "\n",
    "$$\n",
    "\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^0,i^0,d^0)\\phi_5(s^0,i^0) + \\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^0,i^0,d^0)\\phi_5(s^1,i^0)\n",
    "$$\n",
    "\n",
    "This involves 7 calculations. Hmm, but if we look closely, most of this is the same - only $\\phi_5(\\cdots)$ changes. Oh, duh, it factors! It's the same as:\n",
    "\n",
    "$$\n",
    "\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^0,i^0,d^0)\\big(\\phi_5(s^0,i^1) + \\phi_5(s^1,i^1)\\big)\n",
    "$$\n",
    "\n",
    "Which is only 4 calculations. Nice!\n",
    "\n",
    "This silliness is to highlight something painfully important: **factoring summation of products into products of sums** saves us compute, big time.\n",
    "\n",
    "Now, let's see caching in action. Let's pretend we are back to naively calculating this sum. First, we multiply out the first row. We are right about to begin calculation then second row when we realize: we've already calculated the first four terms of that product! Ah, so if we could *cache results* and identify chunks we've already seen, we can save more compute. That point on *identifying* chunks is important - we'll get there.\n",
    "\n",
    "These two ideas, factoring and caching, sit at the foundation of every exact inference algorithm. So, I believe we're ready to discuss one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Elimination (VE)\n",
    "\n",
    "There's one simple factoring trick that enables all of VE. Since it's important, I'll write it out generally. Let's say we'd like to calculate $rs(\\mathbf{x})$, corresponding to an assignment for some $\\mathbf{X}$ from $\\mathcal{X}$. Let's also that $\\mathbf{Z}$ is all the *other* RVs not in $\\mathbf{X}$ and $Z_0 \\in \\mathbf{Z}$ is a variable we've picked out. Further, let's also pretend there are only two factors, $\\phi_1(\\mathbf{D}_1)$ and $\\phi_2(\\mathbf{D}_2)$, where $\\mathbf{D}_1 \\cup \\mathbf{D}_2 = \\mathcal{X}$, and $Z_0$ is not in $\\mathbf{D}_2$. Then we'd like to calculate:\n",
    "\n",
    "$$\n",
    "rs(\\mathbf{x}) = \\sum_{\\mathbf{Z}} \\phi_1(\\mathbf{D}_1) \\phi_2(\\mathbf{D}_2)\n",
    "$$\n",
    "\n",
    "(It's hidden from this notation, but if an $X_j$ from $\\mathbf{X}$ is in one of these $\\mathbf{D}_i$ sets, then that RV takes on the $x_j$ assignment in $\\phi_i(\\mathbf{D}_i)$. All other RVs ($\\mathbf{Z}$) are summed over.)\n",
    "\n",
    "Since $Z_0$ is not in $\\mathbf{D}_2$, then the algebra gods permit:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(\\mathbf{x}) =& \\sum_{\\mathbf{Z}} \\phi_1(\\mathbf{D}_1) \\phi_2(\\mathbf{D}_2) \\\\\n",
    " =& \\sum_{\\mathbf{Z}\\setminus \\{Z_0\\}} \\Big[\\phi_2(\\mathbf{D}_2) \\sum_{Z_0} \\Big[\\phi_1(\\mathbf{D}_1) \\Big]\\Big]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This means you may 'push' summation signs inside product signs, so long as the expressions to the left don't involve the RV of that summation sign. \n",
    "\n",
    "(Going forward, I'll drop those big brackets. They're to demonstrate that the right summation is a term *inside* the left summation. It's *not* the product of two summations.)\n",
    "\n",
    "The thing to look out for is that $\\sum_{Z_0} \\phi_1(\\mathbf{D}_1)$ is a new function that *doesn't* involve $Z_0$. What remains is a function of $\\mathbf{D}_1 \\setminus \\{Z_0\\}$. In effect, we've 'eliminated' $Z_0$. After the summation is done, you could say we've replaced $\\phi_1(\\cdots)$ with a new, slightly simpler, factor. We'll use $\\tau_{Z_0}(\\cdots)$ to refer to the factor you get when you eliminate $Z_0$. We'll call them 'elimination factors'.\n",
    "\n",
    "Now you might be thinking: \"Uh, but there aren't two factors in general.\" Yes, but the principle still works. Just think of $\\phi_1(\\mathbf{D}_1)$ as the product of all factors that involve $Z_0$ and $\\phi_2(\\mathbf{D}_2)$ as the product of all those that don't.\n",
    "\n",
    "With that, you can do this repeatedly, pushing sums inside products, eliminating everything in $\\mathbf{Z}$ and obtaining your answer.\n",
    "\n",
    "But what about caching? That plays a role, but it'll be easier to see in an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of Variable Elimination\n",
    "\n",
    "Let's compute that familiar $rs(c^1)$ guy, which is the sum of the rows of the above Gibbs Table. It may be written:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1) & = \\sum_D \\sum_G \\sum_S \\sum_I \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(I)\\phi_4(G,I,D)\\phi_5(S,I)  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's arbitrarily decide to elimate $S$, then $I$, then $D$ and then $G$ (this is called the 'elimination order'). At this point, I blindly follow the rule for manipulating symbols: push sums inside products until all factors to the right involve the RV of that summation sign. It's easy - give it a shot. You'd get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1) & = \\phi_1(c^1) \\sum_G  \\sum_D\\phi_2(c^1,D) \\sum_I \\phi_3(I)\\phi_4(G,I,D) \\sum_S \\phi_5(S,I) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now let's point out all those elimination factors.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_1(c^1) \\sum_G  \\sum_D\\phi_2(c^1,D) \\sum_I \\phi_3(I)\\phi_4(G,I,D) \\underbrace{\\sum_S \\phi_5(S,I)}_{\\tau_S(I)} \\\\\n",
    "\\phi_1(c^1) \\sum_G  \\sum_D\\phi_2(c^1,D) \\underbrace{\\sum_I \\phi_3(I)\\phi_4(G,I,D) \\tau_S(I)}_{\\tau_I(G,D)} \\\\\n",
    "\\phi_1(c^1) \\sum_G  \\underbrace{\\sum_D\\phi_2(c^1,D) \\tau_I(G,D)}_{\\tau_D(G)} \\\\\n",
    "\\phi_1(c^1) \\underbrace{\\sum_G  \\tau_D(G)}_{\\tau_G} \\\\\n",
    "\\phi_1(c^1) \\tau_G \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(When I omit '()' in front of $\\tau$, that means this function happens to be just one number.)\n",
    "\n",
    "To see why caching helps, imagine coding this up. You tell the computer that so-and-so elimination factor is a summation across this-and-that factors, just as you see in the above cascade. Then you say, 'OK computer, calculate $\\phi_1(c^1) \\tau_G$' and it begins calculating this recursive mess. But in doing so, it'll sometimes come across calculation it's seen before. For example, consider that'll we'll have to calculate $\\tau_I(g^0,d^0)$, $\\tau_I(g^0,d^1)$, $\\tau_I(g^1,d^0)$ and $\\tau_I(g^1,d^1)$. All of these rely on knowing the numbers $\\tau_S(i^0)$ and $\\tau_S(i^1)$. Well, the moment we figured out $\\tau_I(g^0,d^0)$, we must have calculated those numbers. So when it comes to the next calculation involving $\\tau_S(I)$ (which might be $\\tau_I(g^0,d^1)$), don't recalculate them. Since we're assuming they're cached, just look them up.\n",
    "\n",
    "Also, this gives us a means for *identifying* things we've calculated before. It's just the input-output mappings of our eliminations factors.\n",
    "\n",
    "So we see, VE exploits factorization and caching to efficiently calculate our answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the MAP queries?\n",
    "\n",
    "Fortunately, we can leverage our previous knowledge to substantial shorten the length of this explanation.\n",
    "\n",
    "First, we restrict our attention to MAP queries where $\\mathbf{E}$ and $\\mathbf{Y}$ together make up the whole of $\\mathcal{X}$. If this isn't the case, that means there are some RVs $\\mathbf{Z} = \\mathcal{X} - \\{\\mathbf{E},\\mathbf{Y}\\}$ and our query really is:\n",
    "\n",
    "$$\n",
    "\\textrm{argmax}_\\mathbf{Y}P_M(\\mathbf{Y}|\\mathbf{e})=\\textrm{argmax}_\\mathbf{Y}\\sum_\\mathbf{Z}P_M(\\mathbf{Y},\\mathbf{Z}|\\mathbf{e})\n",
    "$$\n",
    "\n",
    "This mixture of maxes and sums make the problem *much* harder - so hard that efficient algorithms for it *don't even exist*. These are actually called 'marginal-MAP queries' and our approach here will be to run away.\n",
    "\n",
    "So we'll focus on the case where there are no RVs in $\\mathbf{Z}$. In this case, Bayes Theorem tells us that this conditional MAP assignment maximize their joint assignment:\n",
    "\n",
    "$$\n",
    "\\textrm{argmax}_\\mathbf{Y}P_M(\\mathbf{Y}|\\mathbf{e}) = \\textrm{argmax}_\\mathbf{Y}P_M(\\mathbf{Y},\\mathbf{e})\n",
    "$$\n",
    "\n",
    "Further, determining the answer for the *unnormalized* distribution gives us the same answer:\n",
    "\n",
    "$$\n",
    "\\textrm{argmax}_\\mathbf{Y}P_M(\\mathbf{Y},\\mathbf{e}) = \\textrm{argmax}_\\mathbf{Y}\\tilde{P}_M(\\mathbf{Y},\\mathbf{e})\n",
    "$$\n",
    "\n",
    "This means that our task is to filter the Gibbs Table to rows consistent with $\\mathbf{e}$ and to find the row (assignment) with the maximum $\\tilde{P}_M(\\mathbf{y},\\mathbf{e})$. We'll assume that if we deterine a maximum, we'll keep track of the assignment[2]. That way, we may speak in terms of the max-value. Since that value is a product, this is often called a 'max-product' algorithm[3].\n",
    "\n",
    "To keep in line with our previous explanation, let's call this function $mr(\\cdots)$ for 'max-row':\n",
    "\n",
    "$$\n",
    "mr(\\mathbf{e}) = \\textrm{max}_\\mathbf{Y}\\tilde{P}_M(\\mathbf{Y},\\mathbf{e})\n",
    "$$\n",
    "\n",
    "At this point, some algorithms diverge in their approach. But let's stick with one that's very similar to what we've seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Elimination for MAP queries\n",
    "\n",
    "There is an analogous factor tricking in the case of MAP queries.\n",
    "\n",
    "Let's say $Y_0$ is a variable from $\\mathbf{Y}$ we'd like to eliminate. Once again, let's pretend there are only two factors, $\\phi_1(\\mathbf{D}_1)$ and $\\phi_2(\\mathbf{D}_2)$, where $\\mathbf{D}_1 \\cup \\mathbf{D}_2 = \\mathcal{X}$, and $Y_0$ is not in $\\mathbf{D}_2$. With this setup, we are determining:\n",
    "\n",
    "$$\n",
    "mr(\\mathbf{e}) = \\textrm{max}_\\mathbf{Y} \\phi_1(\\mathbf{D}_1) \\phi_2(\\mathbf{D}_2)\n",
    "$$\n",
    "\n",
    "Once again, the algebra gods save us and we discover their gift:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "mr(\\mathbf{e}) & = \\textrm{max}_\\mathbf{Y} \\phi_1(\\mathbf{D}_1) \\phi_2(\\mathbf{D}_2) \\\\\n",
    " &= \\textrm{max}_{\\mathbf{Y}\\setminus \\{Y_0\\}} \\Big[\\phi_2(\\mathbf{D}_2) \\textrm{max}_{Y_0} \\Big[\\phi_1(\\mathbf{D}_1) \\Big]\\Big]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And from here, we may proceed *exactly as we did in the case of conditional probabilities*. It's literally as simple as replacing sums with maxes! Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It gets complicated\n",
    "\n",
    "With the details of VE understood, we can briefly discuss an otherwise terrifying algorithm: **the Junction Tree algorithm.** \n",
    "\n",
    "Typically, we aren't concerned with *only* one probability/map query, which would be specific to one observation $\\mathbf{e}$ and one set of RVs $\\mathbf{Y}$. If we were to use VE to address many queries, we'd have to rerun VE for each one. However, it's likely that many of these queries involve identical elimination factors. The Junction Tree algorithm is a super clever way of organization our graph into a data structure that allows us to identify beforehand which calculations are common to which queries. That data structure is called a Clique Tree. It's a tree whereby nodes correspond to groups of RVs and edges correspond to intersections of RVs between these groups. We compile this tree before hand, incurring the cost of approximately one VE run, and then we can answer a wider range of probability queries. In effect, it's another layer of caching results for later reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "The issue with exact inference is that it's *exact* and such perfection comes with a price. Fortunately, there're *approximate methods* which make that trade. I'll be explaining two approaches here:\n",
    "\n",
    "[3] What is Variance Inference? [link] (Posting on DATE)\n",
    "\n",
    "[4] How are Monte Carlo methods used to perform inference in Probabilistic Graphical Models? [link] (Posting on DATE)\n",
    "\n",
    "But maybe you're satisfied and would like to see how we learns parameters from data. I'll have two more options for you:\n",
    "\n",
    "[5] How are the parameters of a Bayesian Network learned? [link] (Posting on DATE)\n",
    "\n",
    "[6] How are the parameters of a Markov Network learned? [link] (Posting on DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[1] Algorithmically, this means we need to set up a 'traceback'.\n",
    "\n",
    "[2] Since you could take the log of this product and maximize that *sum* to yield the same answer, algorithms that accomplish this are often called 'max-sum'.\n",
    "\n",
    "### Sources\n",
    "\n",
    "[1] Koller, Daphne; Friedman, Nir. Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning series). The MIT Press. I owe the effective notation of this answer to that book, along with much of my understanding of this subject.\n",
    "\n",
    "[2] Murphy, Kevin. Machine Learning: A Probabilistic Perspective. The MIT Press. The idea to show VE as that  cascade of equations came from this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for producing tables in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^0$ & $g^0$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^0,i^0,d^0)\\phi_5(s^0,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^0$ & $g^0$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^0,i^0,d^0)\\phi_5(s^1,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^0$ & $g^1$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^1,i^0,d^0)\\phi_5(s^0,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^0$ & $g^1$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^0)\\phi_4(g^1,i^0,d^0)\\phi_5(s^1,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^1$ & $g^0$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^1)\\phi_4(g^0,i^1,d^0)\\phi_5(s^0,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^1$ & $g^0$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^1)\\phi_4(g^0,i^1,d^0)\\phi_5(s^1,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^1$ & $g^1$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^1)\\phi_4(g^1,i^1,d^0)\\phi_5(s^0,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^0$ & $i^1$ & $g^1$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^0)\\phi_3(i^1)\\phi_4(g^1,i^1,d^0)\\phi_5(s^1,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^0$ & $g^0$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^0)\\phi_4(g^0,i^0,d^1)\\phi_5(s^0,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^0$ & $g^0$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^0)\\phi_4(g^0,i^0,d^1)\\phi_5(s^1,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^0$ & $g^1$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^0)\\phi_4(g^1,i^0,d^1)\\phi_5(s^0,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^0$ & $g^1$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^0)\\phi_4(g^1,i^0,d^1)\\phi_5(s^1,i^0) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^1$ & $g^0$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^1)\\phi_4(g^0,i^1,d^1)\\phi_5(s^0,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^1$ & $g^0$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^1)\\phi_4(g^0,i^1,d^1)\\phi_5(s^1,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^1$ & $g^1$ & $s^0$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^1)\\phi_4(g^1,i^1,d^1)\\phi_5(s^0,i^1) $ \\\\\n",
      "\\hline\n",
      "\\cellcolor{gray!25}$c^1$ & $d^1$ & $i^1$ & $g^1$ & $s^1$ & $\\phi_1(c^1)\\phi_2(c^1,d^1)\\phi_3(i^1)\\phi_4(g^1,i^1,d^1)\\phi_5(s^1,i^1) $ \\\\\n",
      "\\hline\n"
     ]
    }
   ],
   "source": [
    "def add_assign(let,val):\n",
    "    return '${0}^{1}$'.format(let,str(val))\n",
    "    \n",
    "letters = ['c','d','i','g','s']\n",
    "\n",
    "def make_prod_str(C,D,I,G,S, whichshade = []):\n",
    "    \n",
    "    vals = [el[-1] for el in [C,D,I,G,S]]\n",
    "    \n",
    "    assign = ' & '.join([add_assign(let,val) for (let,val) in zip(letters,vals)])\n",
    "    \n",
    "    and_locs = [m.start() for m in re.finditer('&', assign)]\n",
    "    \n",
    "    col_string = r'\\cellcolor{gray!25}'\n",
    "    \n",
    "    advance = 0\n",
    "        \n",
    "    for ws in whichshade:\n",
    "        if ws == 0:\n",
    "            assign = col_string + assign\n",
    "        else:\n",
    "            loc = and_locs[ws-1]\n",
    "            assign = assign[:loc+advance+1] + col_string + assign[advance+loc+1:]\n",
    "        advance += len(col_string)\n",
    "        \n",
    "    prod = '\\phi_1({0})\\phi_2({0},{1})\\phi_3({2})\\phi_4({3},{2},{1})\\phi_5({4},{2})'.format(C,D,I,G,S)\n",
    "    \n",
    "    return assign + ' & $' + prod + r' $ \\\\'\n",
    "    \n",
    "    \n",
    "def print_rows(C=[0,1],D=[0,1],I=[0,1],G=[0,1],S=[0,1]):\n",
    "    \n",
    "    which_shade = np.where([el != [0,1] for el in [C,D,I,G,S]],range(5),np.nan)\n",
    "    \n",
    "    which_shade = list(pd.Series(which_shade).dropna())\n",
    "    which_shade = [int(el) for el in which_shade]\n",
    "    \n",
    "    for c in C:\n",
    "        for d in D:\n",
    "            for i in I:\n",
    "                for g in G:\n",
    "                    for s in S:\n",
    "                        print(make_prod_str('c^'+str(c),\n",
    "                                            'd^'+str(d),\n",
    "                                            'i^'+str(i),\n",
    "                                            'g^'+str(g),\n",
    "                                            's^'+str(s),which_shade))\n",
    "                        print('\\hline')\n",
    "    \n",
    "print_rows(C=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap - VE\n",
    "\n",
    "[NOT SURE HOW DETAILED THIS EXPLANATION SHOULD BE]\n",
    "\n",
    "Certainly! First of all, why are we multiplying $\\phi_1(c^1)$ for each row, when it's the same thing each time? Instead, we could sum up each row without that multiplication and then multiply that number by this sum at the end. That would save us 7 calculations! Same with $\\phi_1(i^1)$.\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1,i^1) & = \\sum_D \\sum_G \\sum_S \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(i^1)\\phi_4(G,i^1,D)\\phi_5(S,i^1) \\\\\n",
    "& =  \\phi_1(c^1) \\phi_3(i^1) \\sum_D \\sum_G \\sum_S \\phi_2(c^1,D)\\phi_4(G,i^1,D)\\phi_5(S,i^1) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What else can we do? Well, $S$ appears in only one factor - maybe that's helpful. We are simply summing over the product of every assignment of $S$ with every assignment of $D$ and $G$. We could picture that as a sum across a grid:\n",
    "\n",
    "But maybe there's something special going on with those because those are own observed RVs. What about $S$? Hmm, it's only involved in one factor. Beyond that, all that's happening is that.. for every assignment of $S$, it's\n",
    "\n",
    "[END OF EXPLANATION]\n",
    "\n",
    "[GENERAL VE RULE]\n",
    "\n",
    "Let's say we trying to calculate $rs(\\mathbf{x})$ where $\\mathbf{X}$ is any subset of RVs of $\\mathcal{X}$[1]. Let $\\mathbf{Z}$ be all the other RVs of $\\mathcal{X}$. Then, we are calculating:\n",
    "\n",
    "$$\n",
    "rs(\\mathbf{x}) = \\sum_\\mathbf{Z} \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i)\n",
    "$$\n",
    "\n",
    "It's hidden from this notation, but if an $X_j$ from $\\mathbf{X}$ is in one of these $\\mathbf{D}_i$ sets, then that RV takes on the $x_j$ assignment in $\\phi_i(\\mathbf{D}_i)$. All other RVs ($\\mathbf{Z}$) are summed over.\n",
    "\n",
    "So, let's say we'd like to eliminate $Z_0$. To do so, let's split our set of factors, $\\{\\phi_i(\\cdots)\\}$, into two groups. Those for which $Z_0$ appears and those for which it does not:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi_{Z_0} = & \\{\\phi_i(\\cdots) \\textrm{ if } Z_0 \\textrm{ is in } \\mathbf{D}_i\\} \\\\\n",
    "\\Phi_{\\overline{Z_0}} = & \\{\\phi_i(\\cdots) \\textrm{ if } Z_0 \\textrm{ is not in } \\mathbf{D}_i\\} \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now let $\\mathbf{Z}^*$ be $\\mathbf{Z}$ without $Z_0$: $\\mathbf{Z}^* = \\mathbf{Z} \\setminus \\{Z_0\\}$. With that, it's purely a matter of algebra to realize:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(\\mathbf{x}) = & \\sum_\\mathbf{Z} \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i) \\\\\n",
    "= & \\sum_{\\mathbf{Z}^*} \\sum_{Z_0} \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i) \\\\\n",
    "= & \\sum_{\\mathbf{Z}^*} \\prod_{\\phi_i\\in \\Phi_{\\overline{Z_0}}} \\phi_i(\\mathbf{D}_i) \\sum_{Z_0} \\prod_{\\phi_j\\in \\Phi_{Z_0}} \\phi_j(\\mathbf{D}_j)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "[END OF EXPLANATION]\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1) & = \\sum_D \\sum_G \\sum_S \\sum_I \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(I)\\phi_4(G,I,D)\\phi_5(S,I) \\\\\n",
    "& = \\sum_D \\sum_G  \\sum_I \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(I)\\phi_4(G,I,D) \\sum_S \\phi_5(S,I) \\\\\n",
    "& = \\sum_D \\sum_G   \\phi_1(c^1)\\phi_2(c^1,D) \\sum_I \\phi_3(I)\\phi_4(G,I,D) \\sum_S \\phi_5(S,I) \\\\\n",
    "& =  \\sum_G \\phi_1(c^1)\\sum_D\\phi_2(c^1,D) \\sum_I \\phi_3(I)\\phi_4(G,I,D) \\sum_S \\phi_5(S,I) \\\\\n",
    "& =  \\phi_1(c^1) \\sum_G  \\sum_D\\phi_2(c^1,D) \\sum_I \\phi_3(I)\\phi_4(G,I,D) \\sum_S \\phi_5(S,I) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This algorithm allows us to compute $rs(\\cdots)$ effeciently. To understand it, let's first think about manually calculating $rs(c^1,i^1)$ (chosen arbitrarily). In that case, we want the sum of these rows from the Gibbs table:\n",
    "\n",
    "![title](GibbsTable_filtered.png)\n",
    "\n",
    "Let's write out this sum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1,i^1) & = \\sum_D \\sum_G \\sum_S \\tilde{P}(c^1,D,i^1,G,S) \\\\\n",
    "& = \\sum_D \\sum_G \\sum_S \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(i^1)\\phi_4(G,i^1,D)\\phi_5(S,i^1)  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It's not hard to see that we have 4 multiplications per row to do and 7 summations, for a total of $4\\times8+7=39$ calculations. Can we reduce this?\n",
    "\n",
    "Certainly! It's a mere matter of algebra to realize: We can 'push' the summation sign of a particular RV until all expressions to it's right are only factors involving that RV. So we can do this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(c^1,i^1) & = \\sum_D \\sum_G \\sum_S \\phi_1(c^1)\\phi_2(c^1,D)\\phi_3(i^1)\\phi_4(G,i^1,D)\\phi_5(S,i^1)  \\\\\n",
    "& = \\phi_1(c^1)\\phi_3(i^1)\\Big[\\sum_D \\Big[\\phi_2(c^1,D) \\Big[\\sum_G \\phi_4(G,i^1,D) \\sum_S  \\Big[\\phi_5(S,i^1)\\Big]\\Big]\\Big]  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we'd proceed to calculate this expression from the rightside outward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaing the VE factoring trick\n",
    "\n",
    "Let's say $\\mathbf{Z}$ is some subset of $\\mathcal{X}$ that we're summing over and $Z_0 \\in \\mathbf{Z}$ is a variable we've picked out. \n",
    "Let's also categorize our factors into two groups, depending on whether or not they involve $Z_0$, and multiply those factors together, defining new factors. This is what I mean:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\psi_{Z_0}(\\mathbf{C}_{Z_0}) = \\prod_{i:Z_0 \\in scope(\\phi_i)} \\phi_i(\\mathbf{D}_i) \\\\\n",
    "\\psi_{\\overline{Z_0}}(\\mathbf{C}_{\\overline{Z_0}}) = \\prod_{i:Z_0 \\not\\in scope(\\phi_i)} \\phi_i(\\mathbf{D}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{C}_{Z_0}$ and $\\mathbf{C}_{\\overline{Z_0}}$ are the unions of the associated $\\mathbf{D}_i$. All we're doing is *organizing* factors:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{P}(X_1,\\cdots,X_n) & = \\prod_{\\phi_i\\in \\Phi} \\phi_i(\\mathbf{D}_i) \\\\\n",
    "& = \\psi_{Z_0}(\\mathbf{C}_{Z_0})\\psi_{\\overline{Z_0}}(\\mathbf{C}_{\\overline{Z_0}})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Whew, OK, now let's say $\\mathbf{X}=\\mathbf{x}$ is any assignment[1] of some subset of $\\mathcal{X}$. Then we'd like to calculate:\n",
    "\n",
    "$$\n",
    "rs(\\mathbf{x}) = \\sum_{\\mathbf{Z}} \\psi_{Z_0}(\\mathbf{C}_{Z_0})\\psi_{\\overline{Z_0}}(\\mathbf{C}_{\\overline{Z_0}})\n",
    "$$\n",
    "\n",
    "(It's hidden from this notation, but if an $X_j$ from $\\mathbf{X}$ is in one of these $\\mathbf{C}$ sets, then that RV takes on the $x_j$ assignment in $\\psi(\\mathbf{C})$. All other RVs ($\\mathbf{Z}$) are summed over.)\n",
    "\n",
    "Since $Z_0$ is not in $\\mathbf{C}_{\\overline{Z_0}}$, then the algebra gods permit:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rs(\\mathbf{x}) = & \\sum_{\\mathbf{Z}} \\psi_{Z_0}(\\mathbf{C}_{Z_0})\\psi_{\\overline{Z_0}}(\\mathbf{C}_{\\overline{Z_0}}) \\\\\n",
    " =& \\sum_{\\mathbf{Z}\\setminus \\{Z_0\\}} \\Big[\\psi_{\\overline{Z_0}}(\\mathbf{C}_{\\overline{Z_0}}) \\sum_{Z_0} \\Big[\\psi_{Z_0}(\\mathbf{C}_{Z_0}) \\Big]\\Big]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This means you may 'push' summation signs inside product signs, so long as the expressions to the left don't involve the RV of that summation sign. \n",
    "\n",
    "(Going forward, I'll drop those big brackets. They're to demonstrate that the right summation is a term *inside* the left summation. It's *not* the product of two summations.)\n",
    "\n",
    "The thing to look out for is that $\\sum_{Z_0} \\psi_{Z_0}(\\mathbf{C}_{Z_0})$ is a new function that *doesn't* involve $Z_0$. What remains is a function of $\\mathbf{C}_{Z_0} \\setminus \\{Z_0\\}$. In other words, we've 'eliminated' $Z_0$. After the summation is done, you could say we've replaced $\\psi_{Z_0}(\\cdots)$ with a new, slightly simpler, factor. We'll use $\\tau_{Z_0}(\\cdots)$ to refer to the factor you get when you eliminate $Z_0$. We'll call these 'elimination factors'.\n",
    "\n",
    "With that, you can do this repeatedly, pushing sums inside products, exploiting factorizations, eliminating everything in $\\mathbf{Z}$ and obtaining your answer.\n",
    "\n",
    "But what about caching? That plays a role, but it'll be easier to see in an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
