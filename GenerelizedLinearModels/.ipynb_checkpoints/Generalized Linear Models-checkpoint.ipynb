{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Generalized Linear Models?\n",
    "\n",
    "GLMs were one of those concepts that didn't click until I got lucky with a very specific view on the matter. When I first read the model specification, it looked arbitrary, random and unjustified. Where was it coming from? The subject finally made sense when I saw it from the angle of it's original motiation: a generalization of linear regression. Someone broken linear regression into pieces, generalized each of them and then put it back together, yielding a machine with more nobs. That's really it.\n",
    "\n",
    "So if it worked for me, it can work for you. I'll now share that view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are GLMs?\n",
    "\n",
    "As advertised, we'll start in the bubblegum land of linear regression. There, we assume our dependent variable (a scalar $y$) is normally distributed *given* a linear combination of our independent variables. This can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) & = \\mathcal{N}(y|\\mu,\\sigma^2) \\tag{1}\\\\\n",
    "\\mu & = \\mathbf{x}^T\\boldsymbol{\\beta} \\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I'll call (1) the 'distribution assumption' and (2) the 'parameter relation assumption'. In a GLM, we generalize both.\n",
    "\n",
    "## Generalize the distribution assumption\n",
    "\n",
    "Here, we generalize from:\n",
    "\n",
    "\"$y$ is **normally distributed** given our independent variables\"\n",
    "\n",
    "to\n",
    "\n",
    "\"$y$ has **any distribution within the exponential family** given our independent variables\"\n",
    "\n",
    "This is a good idea, considering the exponential family is pretty awesome (link). To refresh, the *vector* $\\mathbf{y}$ has a distribution from the exponential family if its probability [1] is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{y})=&\\frac{1}{Z(\\boldsymbol{\\theta})}h(\\mathbf{y})\\exp\\Big\\{{T(\\mathbf{y})\\cdot\\boldsymbol{\\theta}\\Big\\}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for some parameter vector $\\boldsymbol{\\theta}$. Our choices of the functions $h(\\cdot)$ and $T(\\cdot)$ will determine which distribution within the exponential family we're in. So we *could* choose them such we get the normal distribution. But a different choice might give us the binomial or poisson or something else. Note we can't choose $Z(\\boldsymbol{\\theta})$ - it's there to ensures our probabilities sum to 1. Our choice of $h(\\cdot)$ and $T(\\cdot)$ will force $Z(\\cdot)$ to be something.\n",
    "\n",
    "One thing to call out is that our dependent variable just went from necessarily a scalar to a vector (which could also be a scalar). So we've sneakily generalized to multi-output predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we do our next generalization...\n",
    "\n",
    "Let's pretend we've chosen $h(\\cdot)$ and $T(\\cdot)$ to recreate linear regression. Now, let's write what we have so far [2]:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) & = \\frac{1}{Z(\\theta)}h(y)\\exp\\Big\\{{T(y)\\cdot\\theta\\Big\\}} \\tag{1}\\\\\n",
    "\\mu & = \\mathbf{x}^T\\boldsymbol{\\beta} \\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice an issue? How does $\\mu$ connect to $\\boldsymbol{\\theta}$? We have to build that bridge. We can get that done with a function we'll call $\\Psi$, which is defined to solve our problem: $\\theta = \\Psi(\\mu)$. In other words, if I select $h(\\cdot)$ and $T(\\cdot)$ to recreate linear regression, $\\Psi(\\cdot)$ will fall out as required. It is purely a result of how we rewrite the normal distribution into the exponential family form.\n",
    "\n",
    "Now un-pretend and let's get back to generalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalize the parameter relation assumption\n",
    "\n",
    "Now we generalize the parameter relation from:\n",
    "\n",
    "\"the scalar mean of $y$ is a linear combination of our independent variables\"\n",
    "\n",
    "to\n",
    "\n",
    "\"**some simple function of our *vector*-**mean is a linear combination of our indendent variables\"\n",
    "\n",
    "That 'simple' function is a known as a link function (call it $g(\\cdot)$), so this may be written as $g(\\boldsymbol{\\mu})=\\mathbf{x}^T\\boldsymbol{\\beta}$. It's a generalization because $g(\\cdot)$ *could* be the identity function and the length of $\\boldsymbol{\\mu}$ *could* be one, yielding what we're familiar with. But it could not and we could land elsewhere. Note that if $\\boldsymbol{\\mu}$ is a vector with length greater than 1, then $\\boldsymbol{\\beta}$ is a matrix.\n",
    "\n",
    "Also, the 'simple' requirement on $g(\\cdot)$ means its invertible, so we may write $\\boldsymbol{\\mu}=g^{-1}(\\mathbf{x}^T\\boldsymbol{\\beta})$\n",
    "\n",
    "So now we have both generalizations. Let's put it all together.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{y}) & = \\frac{1}{Z(\\boldsymbol{\\theta})}h(\\mathbf{y})\\exp\\Big\\{{T(\\mathbf{y})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\tag{1}\\\\\n",
    "\\boldsymbol{\\theta} &  = \\Psi(\\boldsymbol{\\mu}) \\tag{'bridge'}\\\\\n",
    "\\boldsymbol{\\mu} & = g^{-1}(\\mathbf{x}^T\\boldsymbol{\\beta})\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize the parameter relation assumption\n",
    "\n",
    "\n",
    "\n",
    "Now we can focus on\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here, things get a little convoluted, so let's do the following to making things extra clear. Let's express linear regression with our previous generalization incorporated:\n",
    "\n",
    "\n",
    "In linear regression, our parameters were $\\mu$ and $\\sigma^2$. However, if we want to write the normal distribution in the exponential family form, we need a reparametization - a function that will take us from the $\\mu$ and $\\sigma^2$-world to the $\\boldsymbol{\\theta}$-world. Call this function $\\Psi(\\cdot): \\boldsymbol{\\theta} = \\Psi([\\mu, \\sigma^2])$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So incorporate *just* our previous generalization,\n",
    "\n",
    "\n",
    "However, to force linear regression to reside within the exponential family, we require a reparameterization. That is, we need another function $\\Psi$\n",
    "\n",
    "\n",
    "\n",
    "so $\\boldsymbol{\\theta}=[\\mu, \\sigma^2]$ in that case. The relation to the independent variables was created by picking one parameter ($\\mu$) and making it a linear combination of those variables ($\\mu = \\mathbf{x}^T\\boldsymbol{\\beta}$). Let's do something similar. Let's take the general parameter vector $\\boldsymbol{\\theta}$ and split it into two other vectors: $\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_D,\\boldsymbol{\\theta}_C]$. The former group is analogous to $\\mu$ in the sense that it'll **d**epend on our independent variables. The latter is analogous to $\\sigma^2$ in that in won't - we will **c**hoose it exogenously[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[1] 'Probability' isn't correct here. I should say probability density or probability mass.\n",
    "\n",
    "\n",
    "\n",
    "[2] But wait, doesn't $\\sigma^2$ depend on our independent variables? That always ends up as the variance of our residual and those residual depend on our independent variables! Yes, but that routine of estimating the variance after fitting is a convenience specific to linear regression that doesn't generalize. In effect, we can treat $\\sigma^2$ as something we chose exogenously and that idea does generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
