{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the exponential family?\n",
    "\n",
    "Structure:\n",
    "\n",
    "[A lot of exponential family tutorials define the exponential family and show how all the familiar distributions are special cases. This is going for showing the generality of the exponential family to familiar circumstances, but it doesn't show good exponential family reasoning. Meaning it doesn't teach you to reason from the generality of exponential families. Here, we will reason from the data and how the exponential family can handle a wide range.]\n",
    "\n",
    "Talk about the familiar case of estimating the parameters of the normal distribution. Show the vector as a scatter plot and a histogram. \"How would you figure out the normal distribution this came from? Well you'd plug in the empirical mean/variance. The analytic solution is distracting from the general view. What you're actually doing is maximizing this guy (show formula). So you could think (naively) about varying mu/sigma over all values and maximizes the result. (Show a grid of graphs where we vary mu and sigma and show the PDF over the histogram. Highlight the max)\n",
    "\n",
    "Now I'm going to do the *exact same thing*, but I'm going to rewrite some of the algebra and name a few things.\n",
    "\n",
    "(See the Gaussian section of https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf)\n",
    "\n",
    "Create labels on the expanded exponential family-like equation:\n",
    "\n",
    "    1. measurements of our data\n",
    "    2. a function of our parameters that get multiplied by our measurements\n",
    "    3. a function of our parameter that get substracted out\n",
    "    4. a number (the 1/sqrt(2pi))\n",
    "    \n",
    "Now let's relabel [mu/sigma^1, -1/2sigma^2] as [theta1, theta2]. You'd agree that if I know that values of [theta1, theta2], I'd also know the values of mu and sigma^2, yes? (2 equations, 2 variables). So that means I know A(\\mu,\\sigma)\n",
    "\n",
    "(Describe how you can now search over theta1, theta2)\n",
    "\n",
    "This right here is the heart of the exponential family. What's hidden here is the implicit choices we made that led us to the normal distribution, rather than something else. I'll get into that, but now I'll define the exponential family in all it's glory.\n",
    "\n",
    "(Define)\n",
    "\n",
    "(Talk about \\eta(x) and how it's a bit of a trick to get us to know what to sum over.)\n",
    "\n",
    "So when I said 'we suspect it's normally distributed', that's a choice right there! We are basically saying the only thing that matters about the data is:\n",
    "\n",
    "h(x) = 1\n",
    "T(x) = [x,x^2] (or a sum? - actually no, start about reasoning about one data point at a time)\n",
    "\n",
    "But we need, so that means:\n",
    "\n",
    "A(theta) = (something of theta)\n",
    "\n",
    "since this is needed to make sure the distribution sums to 1.\n",
    "\n",
    "Then when we answer the question 'which normal distribution fits best?' we find the theta1 and theta2 that maximize that objective. Now in this specific instance, we *could* rewrite theta1 and theta2 to be our normal parameters? But what's the point? Theta1 and theta2 make more sense in the broader context.\n",
    "\n",
    "Now say you came across data like this (describe 1-0 data). Now let's come at it from the direction of the exponential family. What do you think matters about one piece of data? The only thing I can think of if it's a 1 or not. So let's choose\n",
    "\n",
    "h(x) = 1\n",
    "T(x) = x\n",
    "\n",
    "We need that integrate to 1 situation, so that makes:\n",
    "\n",
    "A(theta) = (something of theta)\n",
    "\n",
    "Great, now find the best theta1 across all the data.\n",
    "\n",
    "\n",
    "\n",
    "We're done (but explain how we just fit a bernoulli).\n",
    "\n",
    "Let's try something harder, what if each data point was the number of bernoulli success out of N trials. So we have M pieces of data like this:\n",
    "\n",
    "[Show binomial data]\n",
    "\n",
    "It's hard to think directly about the measurement of the data that matters, but it might be easier if we think about what's 'underneath' all this. That would look like:\n",
    "\n",
    "[Show one binomial datapoint expanded out into N bernoulli trials]\n",
    "\n",
    "So what's the probability of observing a certain sum? Well we need that many bernoulli trials to go off, but they can go off in any number of (n choose x) ways. So the probability is:\n",
    "\n",
    "(Show binomial pmf in exponential family form)\n",
    "\n",
    "ahh so now we see the point of h(x). It tells us how *big* x is, independent of parameters. So if I have a binomial with N = 6, then the event x = 3 is (n choose x) times larger than the atomic event the exponential is measuring. This is true *despite* our settings of the parameters.\n",
    "\n",
    "Ok, but discrete might be too easy. Let's try something harder. What if I have data like this:\n",
    "\n",
    "[Show exponential data]\n",
    "\n",
    "What measure about this matters? I don't know, hmm. Let's take a tra\n",
    "\n",
    "Hmm, I don't know how to measure this. But maybe I can build it out of stuff I do understand? Expand\n",
    "\n",
    "\n",
    "\n",
    "2. \"and THIS is what the exponential family is about\" it's a general framework \n",
    "\n",
    "2. The procedure mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import norm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# plt.rc('text', usetex=True)\n",
    "# plt.rc('text.latex', preamble=r'\\usepackage{extarrows}')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I'll give a different perspective on the exponential family. If you visit the wiki page, you'll read its definition and its wonderfully general properties. That's all well and good, but it doesn't well represent how it's useful to a data scientist. I'll try to do that here. I'll avoid some typical extensions, because they distract from a good starting point. I'll start with a real simple problem, how the exponential family helps in that circumstance and how it can, in fact, help in more broader circumstances. After all this, you should be able to put this knowledge in your pocket and actually use the exponential family in all it's general glory. Now..\n",
    "\n",
    "[Insert Vegeta picture with the let us begin caption]\n",
    "\n",
    "Let's say my data is a vector of real values scalars $\\{x_{i}\\}_{i=1}^{N}$ and I *suspect* it's normally distributed. The goal is to determine *which* normal distribution it comes from. This might sound trivial - the empirical mean/variance tell us which normal distribution. But that answer is representative of the broader picture. What we are really doing is answering the question:\n",
    "\n",
    "“What are the parameters of the normal distribution that maximize the data I'm seeing?”\n",
    "\n",
    "In the simple case of the normal distribution, we *happen* to have a way to calculate that answer immediately from the data. But we don't always. So let's handicap\n",
    "\n",
    "If you'd like to estimate the parameters of the normal distribution, you do the typical routine: the mean is the average and the variance is the empirical variance\n",
    "\n",
    "If you know how to approach this, forget your approach!\n",
    "\n",
    "Let's define it. We seek to define some probability distribution over a vector of observations. We do that with this magical device:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta})=\\frac{1}{Z(\\boldsymbol{\\theta})}h(\\mathbf{x})e^{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "T(\\mathbf{x}) = & \\left[\\begin{array}{c}\n",
    "x\\\\\n",
    "x^2\n",
    "\\end{array}\\right] \\\\\n",
    "\\boldsymbol{\\theta} = & \\left[\\begin{array}{c}\n",
    "\\frac{\\mu}{\\sigma^2}\\\\\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\end{array}\\right] \\\\\n",
    "h(\\mathbf{x}) = & 1 \\\\\n",
    "\\log Z(\\boldsymbol{\\theta}) = & \\frac{1}{2\\sigma^2}\\mu^2+\\log\\sigma + \\frac{1}{2}\\log (2\\pi)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to determine something about the multinomial.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO GET THE IDEA OF THE MINIMAL REPRESENTATION IN THERE EARLY. SAY THIS IS WHAT IT'S ALL ABOUT. This will fix up the explanation of the multinomial.\n",
    "\n",
    "'It's about sweeping over distributions in the real coordinate system'\n",
    "\n",
    "Talk about how exp() helps us map everything to [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normal\n",
    "N_norm = 1000\n",
    "loc = 3.5\n",
    "scale = 2\n",
    "data_normal = np.random.normal(loc=loc,scale=scale,size=N_norm)\n",
    "\n",
    "#bernoulli\n",
    "N_ber = 1000\n",
    "prob_B = .35\n",
    "data_ber = np.array(['A']*N_ber)\n",
    "data_ber[np.random.uniform(size=N_ber)>prob_B] = 'A'\n",
    "\n",
    "#multinomial\n",
    "N_mult = 1000\n",
    "probs = [.2,.35,.45]\n",
    "sum_mult = 10\n",
    "data_mult = np.random.multinomial(sum_mult,probs,N_mult)\n",
    "\n",
    "def mult_combinations(sum_mult):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for x1 in range(sum_mult+1):\n",
    "        for x2 in range(sum_mult-x1+1):\n",
    "            x3 = sum_mult-(x1+x2)\n",
    "            out.append([x1,x2,x3])\n",
    "            \n",
    "    return np.array(out)\n",
    "    \n",
    "mult_combs = mult_combinations(sum_mult)\n",
    "\n",
    "assert len(mult_combs) == np.math.factorial(12)/(np.math.factorial(10)*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Answer\n",
    "\n",
    "I'd like to give a different perspective on the exponential family. The general pattern of explanation for the exponential family is to give it's definition and then show how familiar distributions fit into that form. This has it's merits, but I'd like to reason from somewhere else: the observation of the data. From here, we say 'the data is like thisa, can we handle it with the exponential family?' As we discover we can handle a diversity of data, we'll see how flexible the exponentail family is.\n",
    "\n",
    "I'll start with a real simple problem, how the exponential family helps in that familiar circumstance and how it can, in fact, help in much broader circumstances. \n",
    "\n",
    "Before dividing in, I'll tell you that this is quite a long answer, but for very good reason. If you can master the exponential family, you immediately understand a wide class of distributions. On a per minute-of-study basis, it's a worthwhile purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's get started. Let's say we've come across a list of continuous numbers that 'look' approximately normally distributed.\n",
    "\n",
    "Our goal is to determine which distribution generated these numbers. That is, we speculate a distribution and determine which parameters of that distribution make the most sense according to the data. In this case, it certainly looks normally distributed, so let's guess that distribution. Then we pick the maximum likelihood parameters (those that 'make the most sense').\n",
    "\n",
    "Now, before you say, 'just use the empirical mean/variance', let's think about exactly what we are doing. We are concerned with finding the parameters $\\mu$ and $\\sigma^2$ that maximize the likelihood of our data. That is, find me $\\mu^*$ and $\\sigma^{2*}$:\n",
    "\n",
    "$$\n",
    "\\mu^*,\\sigma^{2*} = \\textrm{argmax}_{\\mu,\\sigma^2} \\prod_i^N \\mathcal{N}(x_i | \\mu,\\sigma^2)\n",
    "$$\n",
    "\n",
    "The answer happens to be the empirical mean and variance, but that solution doesn't generalize, so forget it! Let's scan for values of $\\mu$ and $\\sigma^2$ until we find a combination that works. That process looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to do the *exact same thing*, but I'm going to rewrite some of the algebra.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{N}(x_i | \\mu,\\sigma^2) = & \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big\\{{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\\Big\\} \\\\\n",
    "= & \\exp\\Big\\{\\frac{\\mu}{\\sigma^2}x_i - \\frac{1}{2\\sigma^2}x_i^2 - \\frac{1}{2\\sigma^2}\\mu^2-\\log\\sigma -\\frac{1}{2}\\log (2\\pi) \\Big\\} \\\\\n",
    "= & \\exp\\Big\\{\\left[\\begin{array}{cc}\n",
    "x_i & x_i^2\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\frac{\\mu}{\\sigma^2}\\\\\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\end{array}\\right] - \\big(\\frac{1}{2\\sigma^2}\\mu^2+\\log\\sigma + \\frac{1}{2}\\log (2\\pi) \\big)\\Big\\} \\\\\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "Now let's label: \n",
    "\n",
    "$$ \\left[\\begin{array}{c}\n",
    "\\frac{\\mu}{\\sigma^2}\\\\\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "You'd agree that if knew $\\theta_1$ and $\\theta_2$, I'd know $\\mu$ and $\\sigma^2$, right?. So let's reason in terms of those variables - instead of searching for $\\mu$ and $\\sigma^2$ that maximize the function, let's look for $\\theta_1$ and $\\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show multiple-plot where we fit different normals to the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's worth stating it again: this is the *exact same thing* as finding the best mean and variance. We just did it in different terms. But the crazy thing is.. nearly every distribution you've heard of can be re-worked into this form. In other words, we can do the equivalent of finding $\\mu^*$ and $\\sigma^{2*}$, but for a huge range of distributions.\n",
    "\n",
    "### So what is the fully general exponential family?\n",
    "\n",
    "According to the exponential family, the probability of a vector $\\mathbf{x}$ according to a parameter vector $\\boldsymbol{\\theta}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta})=&\\frac{1}{Z(\\boldsymbol{\\theta})}h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$Z(\\boldsymbol{\\theta})$ is called the partition function and it's there to ensure that $p(\\mathbf{x}|\\boldsymbol{\\theta})$ sums to 1 over $\\mathbf{x}$. That is\n",
    "\n",
    "$$\n",
    "Z(\\boldsymbol{\\theta}) = \\int h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\nu(d\\mathbf{x})\n",
    "$$\n",
    "\n",
    "$\\nu(d\\mathbf{x})$ refers to the 'measure' of $\\mathbf{x}$. It's there to generalize the idea of 'summing over all possible events' to the correct domain (discrete, continuous or subsets of either). So when we say we 'know' $\\nu(d\\mathbf{x})$, that means we know how to sum over all possible $\\mathbf{x}$'s appropriately. It also may determine the 'volume' of $\\mathbf{x}$, though you can get that work done with $h(\\mathbf{x})$.\n",
    "\n",
    "In fact, let's make that separation and say that's what $h(\\mathbf{x})$ is - it's the volume  of $\\mathbf{x}$. Think of this as the component of $\\mathbf{x}$'s likelihood that *isn't* due to it's parameters. This will become more clear with an example.\n",
    "\n",
    "$T(\\mathbf{x})$ is called the 'vector of *sufficient* statistics'. This is a measurement of our data that is 100% of what we need to determine agreement with $\\boldsymbol{\\theta}$. In other words, if I handed you two vectors ($\\mathbf{x}_1$ and $\\mathbf{x}_2$) that had the same sufficient statistics (so $T(\\mathbf{x}_1) = T(\\mathbf{x}_2)$) then these data points would agree with all parameter vectors equally.[1]\n",
    "\n",
    "There are a few extra details regarding the parameters:\n",
    "\n",
    "1. We'll only consider parameters for which the partition function is finite. If you look at the partition function, it's quite easy to imagine an integral that diverges. This space of 'legal' parameters is called the natural parameter space.\n",
    "\n",
    "2. There *should*[Add FN] be no linear dependencies between the parameters (or sufficient statistics) in this representation. That is to say, you should be free to move *all* elements of $\\boldsymbol{\\theta}$. Said differently, knowing one subset of parameters should never fix the others. If this is true, the representation is said to be *minimal*. There are a few reasons for this. First, you never lose any ability to represent distributions by enforcing it. Second, since we will ultimately be searching over the space of $\\boldsymbol{\\theta}$, we will have the benefit that different $\\boldsymbol{\\theta}$'s always imply different distributions. And lastly (and I discovered this painfully), if it's not true, it can mask $Z(\\boldsymbol{\\theta})$ such that it appears to be zero[2].\n",
    "\n",
    "That last point will guide how we should think: The length of $\\boldsymbol{\\theta}$ will determine how many degrees of freedom we have over our probability distributions. So when we choose $T(\\mathbf{x}_1)$, we need to think carefully first about it's length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exponential family combines nicely across our data\n",
    "\n",
    "Before diving into examples, we can immediately notice one thing useful. In this form, think about how the probability of a sample combines to form the probability of all our data.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_i^N p(\\mathbf{x}_i|\\boldsymbol{\\theta}) & = \\prod_i^N h(\\mathbf{x}_i)\\exp\\Big\\{{T(\\mathbf{x}_i)\\cdot\\boldsymbol{\\theta}-\\log Z(\\boldsymbol{\\theta})\\Big\\}}\\\\\n",
    "& = \\Big(\\prod_i^N h(\\mathbf{x}_i)\\Big) \\exp\\Big\\{{\\big(\\sum_i^N T(\\mathbf{x}_i)\\big)\\cdot\\boldsymbol{\\theta}-N\\log Z(\\boldsymbol{\\theta})\\Big\\}}\\\\\n",
    "& = h_N(\\mathbf{X}) \\exp\\Big\\{{T_N(\\mathbf{X})\\cdot\\boldsymbol{\\theta}-N\\log Z(\\boldsymbol{\\theta})\\Big\\}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Look! That relabeling shows that the probability of *all our data* is just a new distribution in the exponential family. Easy!\n",
    "\n",
    "Since determining the MLE will involve maximizing the log of this, call that $\\mathcal{L}_N(\\boldsymbol{\\theta})$. That is:\n",
    "\n",
    "$$\n",
    "\\log \\prod_i^N p(\\mathbf{x}_i|\\boldsymbol{\\theta}) = \\mathcal{L}_N(\\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, but how does this relate to the normal distribution?\n",
    "\n",
    "So when we fitted our normal distribution, we were quietly making choices with respect to the exponential family form. That is, we were making assertions that implied specific settings to our exponential family. Those assertions were:\n",
    "\n",
    "1. $x$ could be any real valued number: that sets $\\nu(dx)$ which determines how we'll do our integration. That is, we'll integrate over the real line.\n",
    "2. It's normally distributed, which dictates a few things:\n",
    "    * It has two degrees of freedom, so that tells us the length of $T(x)$ and $\\boldsymbol{\\theta}$.\n",
    "    * $T(x) = [x,x^2]$, which means probabilities are influenced by parameters only via a linear relationship with *these* measurements.\n",
    "    * $h(x) = 1$, which means that the difference in likelihood between two $\\mathbf{x}$ values is due entirely the parameters. In other words, all $x$'s have the same size.\n",
    "\n",
    "From here, the definition of the exponential family will dictate the rest. That is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(\\boldsymbol{\\theta}) = & \\int h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\nu(d\\mathbf{x})\\\\\n",
    "= & \\int \\exp\\Big\\{{ \\left[\\begin{array}{cc}\n",
    "x & x^2\\end{array}\\right] \\cdot \\left[\\begin{array}{c}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\n",
    "\\end{array}\\right] \\Big\\}} dx \\\\\n",
    "= & \\exp\\Big\\{ \\frac{-\\theta^2_1}{4\\theta_2} - \\frac{1}{2}\\log(-2\\theta_2) - \\frac{1}{2}\\log(2\\pi)\\Big\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now that we have this, we can determine $\\mathcal{L}_N(\\boldsymbol{\\theta})$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_N(\\boldsymbol{\\theta}) = & \\sum_i^N\\log \\mathcal{N}(x_i | \\mu,\\sigma^2) \\\\\n",
    "= & \\left[\\begin{array}{cc}\n",
    "\\sum_i^N x_i & \\sum_i^N x_i^2\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\n",
    "\\end{array}\\right] - N\\big( \\frac{-\\theta^2_1}{4\\theta_2} - \\frac{1}{2}\\log(-2\\theta_2) - \\frac{1}{2}\\log(2\\pi)\\big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now picking the best $\\theta_1$ and $\\theta_2$ (maximizing $\\mathcal{L}_N(\\boldsymbol{\\theta})$) will be the exact same procedure we did earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's so useful about the generalization?\n",
    "\n",
    "The reason for this rephrasing is it reveals all the remarkable degrees of freedom the exponential family rewards us. We didn't *need* to say $x$ was real valued - that was our choice. We didn't *need* to say the sufficient statistics were $[x,x^2]$, we could have picked anything! So let's use those degrees of freedom in another example. Let's say I come across data like this:\n",
    "\n",
    "[Show a vector with A's and B's as values]\n",
    "\n",
    "[Show a histogram with A/B counts]\n",
    "\n",
    "Hmm, these aren't numbers. We'll let's make some choices:\n",
    "\n",
    "1. $\\nu(dx)$ should mean we sum over the two possible outcomes ($x=A$ or $x=B$)\n",
    "2. I suspect only one degree of freedom is appropriate here. With that, I suggest we use an indicator function:\n",
    "$$\n",
    "    T(x)= \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } x = A\\\\\n",
    "    0              & \\text{if } x = B\n",
    "\\end{cases}\n",
    "$$\n",
    "which can be represented as $\\mathbb{1}[x=A]$\n",
    "3. Outside of something that relates to the parameters, I have no reason to think $x=A$ has a greater volume than $x=B$ (or visa versa), so let's say $h(x)=1$.\n",
    "\n",
    "We've made all our choices - What does this imply about $Z(\\boldsymbol{\\theta})$?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(\\theta) = & \\int h(x)\\exp\\Big\\{{T(x)\\cdot\\theta\\Big\\}} \\nu(dx)\\\\\n",
    "= & \\int \\exp\\Big\\{{\\mathbb{1}[x=A]\\cdot\\theta\\Big\\}} \\nu(dx)\\\\\n",
    "= & \\sum_{x \\in \\{A,B\\}} \\exp\\Big\\{{\\mathbb{1}[x=A]\\cdot\\theta\\Big\\}}\\\\\n",
    "= & \\exp(\\theta) + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And now we can write $p(x|\\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x|\\theta)= &\\frac{1}{Z(\\theta)}h(x)\\exp\\Big\\{{T(x)\\cdot\\theta\\Big\\}}\\\\\n",
    "= &\\frac{1}{\\exp(\\theta) + 1}\\exp\\Big\\{{\\mathbb{1}[x=A]\\cdot\\theta\\Big\\}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or, written out more explicitly:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x=A|\\theta) = &\\frac{\\exp(\\theta)}{\\exp(\\theta) + 1}\\\\\n",
    "p(x=B|\\theta) = &\\frac{1}{\\exp(\\theta) + 1} = 1 - p(x=A|\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since we can choose $\\theta$ to yield any value in [0,1] for $p(x=A|\\theta)$, we see this distributions just assigns a constant probability to the events $x=A$ and $x=B$ - in other words, it's a Bernoulli random variable. \n",
    "\n",
    "So to pick the most likely parameters, we optimize $\\mathcal{L}_N(\\theta)$, just like we did in the case of the normal. Here, there is a simple analytic solution (just set the derivative to zero and see what falls out).\n",
    "\n",
    "So one set of choices led us directly to the normal distribution while another set led us to the Bernoulli distribution. What else can we generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try something harder!\n",
    "\n",
    "What if we came across data like this:\n",
    "\n",
    "[Show multinomial data]\n",
    "\n",
    "Well this is odd. Now $\\mathbf{x}$ is a length 3 vector and it's got this unusual constraint where the elements sum to 5. Ok, deep breath - let's start from our familiar spot:\n",
    "\n",
    "1. How should we think about $\\nu(d\\mathbf{x})$? How should we think about all possible events that we should sum over? Said differently, what are the legal observations you could make for this data? Well it's any 3 nonnegative integers such that their sum is 5. Here, I'll write them out for you:\n",
    "\n",
    "    So I just need to sum over all these. Let's call this set $\\mathcal{X}$.\n",
    "\n",
    "2. How many degrees of freedom should our modeling allow? Well I suspect there'll be a parameter for each column, but the constraint that their sum is 5 will subtract one, so le't say there are 2 degrees of freedom.\n",
    "\n",
    "3. What is $T(\\mathbf{x})$? The most natural thing I can think of is the data itself, so let's go with the first two elements of $\\mathbf{x}$. We aren't losing information regarding the last element, since our distribution will also 'know' that the sum is 5.\n",
    "\n",
    "4. What is $h(\\mathbf{x})$? In other words, should one observation of $\\mathbf{x}$ ever be considered more likely than another, regardless of what parameter vector $\\boldsymbol{\\theta}$ we have? From this angle, I have no clue, but it's not simple enough for me to say $h(\\mathbf{x})=1$. To crystalize our understanding, let's think about how we would generate $\\mathbf{x}$ ourselves. One way is to make 5 draws (from some distribution I don't yet know) and then aggregate the results, like this:\n",
    "\n",
    "    [Show mapping of things like A,A,B,A,C --> A = 3, B = 1, C = 1]\n",
    "    \n",
    "    The useful thing here is that all possible sequences will map to all events of $\\mathcal{X}$. Now from this angle, do some observations seem more likely then others (even though we can't reference the distributed that generates a single draw)? Well, some observations are mapped to by more sequences than others. For example, the only thing that maps to [5,0,0] is [A,A,A,A,A] while [4,1,0] is mapped to by 5 sequences ($[B,A,A,A,A],[A,B,A,A,A],\\cdots,[A,A,A,A,B]$). So it seems the latter observations is 5 times bigger than the former. Notice this is true *despite not knowing the parameters*. So let's make $h(\\mathbf{x})$ the number of sequences that map to $\\mathbf{x}$. If you remember your combinatorics, that is:\n",
    "    \n",
    "    $$\n",
    "    h(\\mathbf{x}) = \\frac{5!}{(x_1!)(x_2!)(x_3!)}\n",
    "    $$\n",
    "    \n",
    "Ok, we've made all our choices so we should be good to go. The partition function is this guy:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(\\boldsymbol{\\theta}) = & \\int h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\nu(d\\mathbf{x})\\\\\n",
    "= & \\sum_{\\mathbf{x}\\in \\mathcal{X}} \\frac{5!}{(x_1!)(x_2!)(x_3!)}\\exp\\Big\\{\\mathbf{x}_{:2}\\cdot\\boldsymbol{\\theta}\\Big\\}\\\\\n",
    "= &  (\\exp(\\theta_1) + \\exp(\\theta_2) + 1)^5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If you're wondering how I got from the second line to the third, the answer is.. I don't know. I just, from an entirely separate arena, happen to know that's true. So now we may write the expression for the probability:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta}) = & \\frac{1}{(\\exp(\\theta_1) + \\exp(\\theta_2) + 1)^5}\\frac{5!}{(x_1!)(x_2!)(x_3!)}\\exp\\Big\\{\\mathbf{x}_{:2}\\cdot\\boldsymbol{\\theta}\\Big\\}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "At this point, we have everything we need to optimize $\\mathcal{L}_N(\\boldsymbol{\\theta})$. But if you're feeling uneasy with this unusual form that's followed from our rather basic choices, I can tell you something relaxing. This is actually identical to this form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}|\\boldsymbol{\\theta})= \\frac{5!}{(x_1!)(x_2!)(x_3!)}p_1^{x_1}p_2^{x_2}p_3^{x_3}\n",
    "$$\n",
    "\n",
    "where $\\theta$ has been rewritten in terms of probabilities $p_1$, $p_2$ and $p_3$. These probabilities refer to the chances of a particular draw when we were generating our sequences earlier. In other words, this was just the multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm not impressed\n",
    "\n",
    "These examples should communicate the flexibility of the exponential family. To cover more ground with fewer words, I'll just show you a taste of the diversity of distributions given by different choices of $h(\\mathbf{x}$ and $T(\\mathbf{x}$. From the wiki page:\n",
    "\n",
    "![title](ExpDiversity.png)\n",
    "\n",
    "Look at that. That sample is almost a college course worth of distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Any other insights?\n",
    "\n",
    "There is some intuition that remains to be had. When we 'pick the most appropriate $\\boldsymbol{\\theta}$', we were maximizing this guy:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\prod_i^N p(\\mathbf{x}_i|\\boldsymbol{\\theta}) & = \\log h_N(\\mathbf{X}) + T_N(\\mathbf{X})\\cdot\\boldsymbol{\\theta}-N\\log Z(\\boldsymbol{\\theta})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(which we called $\\mathcal{L}_N(\\boldsymbol{\\theta})$). Up until this point, we have been optimizing this rather blindly. But in reality, the first step is to compute the gradient, so let's start there:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}_N(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = & T_N(\\mathbf{X}) - N \\frac{\\partial \\log Z(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ok, we got this. Let's figure out $\\frac{\\log \\partial Z(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\log \\partial Z(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = & \\frac{1}{Z(\\boldsymbol{\\theta})}\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\int h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\nu(d\\mathbf{x})\\\\\n",
    "= & \\frac{1}{Z(\\boldsymbol{\\theta})}\\int T(\\mathbf{x}) h(\\mathbf{x})\\exp\\Big\\{{T(\\mathbf{x})\\cdot\\boldsymbol{\\theta}\\Big\\}} \\nu(d\\mathbf{x})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It's a bit sneaky, but the integrand just became a vector (due to multiplication by $T(\\mathbf{x})$). Now if we look closely, it's just a probability weighted average of $T(\\mathbf{x})$. In other words, it's the expectation of $T(\\mathbf{x})$ under the parameters dictated by $\\boldsymbol{\\theta}$. That is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\log \\partial Z(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = & \\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x}|\\boldsymbol{\\theta})}\\big[ T(\\mathbf{x}) \\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So a rescaled version (scaling doesn't matter) of our gradient is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{1}{N} \\frac{\\partial \\mathcal{L}_N(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = & \\frac{1}{N} T_N(\\mathbf{X}) - \\mathbb{E}\\big[ T(\\mathbf{x}) \\big] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "How wild is that!? We have this super general optimization problem and out pops this very short, very intuitive expression for the gradient. The gradient tells us what direction (in the space of $\\boldsymbol{\\theta}$'s) to move. This says 'move in the direction that reduces the biggest differences between our calculated/observed sufficient statistics and those expected under the current choice of $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Another way to see this is that at the maximum, the gradient is zero. So when we found our best parameters, those parameters will dictate an expectation of sufficient statistics that are equal to our observed averaged sufficient statistics.\n",
    "\n",
    "The whole point of this is: when we are searching around for the most appropriate $\\boldsymbol{\\theta}$, we have an intuition for what's dictating our direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the catch?\n",
    "\n",
    "This, unfortunately, isn't a cure all solution. There is in fact a catch - it's the normalizer $Z(\\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extensions:\n",
    "\n",
    "1. Mention GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Where'd you learn about this?\n",
    "\n",
    "There were three great references in producing this:\n",
    "\n",
    "1. Kevin Murphy's Machine Learning: A Probabilistic Perspective (chapter 9)\n",
    "2. This pdf floating around online: https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf Which I suspect is by Michael I Jordan.\n",
    "3. The wiki page and the army of smart people who organized it. Wikipedia - you're awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[1] I realize 'agreement with parameters' is a bit vague. The actual definition is .. [ELAB]\n",
    "[2] I don't understand why exactly it's true, but it's mentioned here: https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf\n",
    "[3] It might be confusing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# M = 5\n",
    "# x_len = 3\n",
    "\n",
    "# def n_choose_subsets(x):\n",
    "#     numerator = np.math.factorial(np.sum(x))\n",
    "#     denominator = np.prod(np.array([np.math.factorial(xi) for xi in x]))\n",
    "    \n",
    "#     return (numerator/denominator)\n",
    "\n",
    "# def prob_unnorm(x,theta):\n",
    "    \n",
    "#     return n_choose_subsets(x)*np.exp(np.sum(x[:2]*theta))\n",
    "\n",
    "# def partition(theta):\n",
    "    \n",
    "#     count = 0\n",
    "#     Z = 0\n",
    "#     for x1 in range(M+1):\n",
    "#         for x2 in range(M-x1+1):\n",
    "#             x3 = M-(x1+x2)\n",
    "#             x = np.array([x1,x2,x3])\n",
    "#             count += 1\n",
    "#             Z += prob_unnorm(x,theta)\n",
    "            \n",
    "#     return Z, count\n",
    "            \n",
    "# count_analytic = np.math.factorial(M+x_len-1)/(np.math.factorial(M)*np.math.factorial((M+x_len-1)-M))\n",
    "\n",
    "# def my_prob(x, theta):\n",
    "#     Z, count = partition(theta)\n",
    "#     return prob_unnorm(x,theta)/Z\n",
    "\n",
    "# def their_prob(x,theta):\n",
    "#     return n_choose_subsets(x)*np.exp(np.sum(x*theta))\n",
    "\n",
    "\n",
    "# # def theta_to_prob(theta):\n",
    "# #     p1 =\n",
    "\n",
    "# theta = np.array([.2,.1])\n",
    "# p3 = 1/(np.exp(theta[0])+np.exp(theta[1])+1)\n",
    "# # x = [2,2,1]\n",
    "# # print(my_prob(x, theta))\n",
    "# # print(their_prob(x,theta))\n",
    "# print(np.log(partition(theta)))\n",
    "# # print(M*np.log(1/p3))\n",
    "# val = (np.exp(theta[0])+np.exp(theta[1])+1)\n",
    "# print(M*np.log(val))\n",
    "# print(val**M)\n",
    "# print(partition(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def Exponential1DReal(hx,tx, theta):\n",
    "    \n",
    "#     assert len(tx(0)) == len(theta)\n",
    "     \n",
    "#     def prob_unnorm(x):\n",
    "#         return hx(x)*np.exp(np.sum(tx(x)*theta))\n",
    "    \n",
    "#     Z =  integrate.quad(prob_unnorm, -np.inf, np.inf)[0]\n",
    "\n",
    "    \n",
    "#     def prob(x):\n",
    "#         return prob_unnorm(x)\n",
    "    \n",
    "#     return prob\n",
    "\n",
    "# mu = .1\n",
    "# sigma2 = 2\n",
    "# # theta1 = mu/sigma2\n",
    "# # theta2 = -1/(2*sigma2)\n",
    "# theta1 = -1\n",
    "\n",
    "# # tx = lambda x : np.array([np.log(x),np.log(x)**2])\n",
    "# tx = lambda x : np.array([x])\n",
    "# hx = lambda x : 1\n",
    "# theta = np.array([theta1])\n",
    "# my_norm = Exponential1DReal(hx,tx, theta)\n",
    "\n",
    "# real_line = np.linspace(0,5,100)\n",
    "\n",
    "# # integrate.quad(my_norm, -np.inf, np.inf)[0]\n",
    "\n",
    "# # # def normZ(theta1,theta2):\n",
    "# # #     logZ = -(theta1**2)/(4*theta2) - (1/2)*np.log(-2*theta2) - (1/2)*np.log(2*np.pi)\n",
    "# # #     return np.exp(logZ)\n",
    "\n",
    "# for i in real_line:\n",
    "#     print('--------')\n",
    "#     print(my_norm(i))\n",
    "# # #     print(normZ(theta1,theta2))\n",
    "# #     print(norm.pdf(i,mu,np.sqrt(sigma2)))\n",
    "\n",
    "# pd.Series([my_norm(i) for i in real_line],index=real_line).plot(kind='line')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# # rv = norm()\n",
    "# #     my_norm    \n",
    "# # # x = 0\n",
    "\n",
    "# # # hx(x)*np.exp(np.sum(tx(x)*theta))\n",
    "# # # tx(x)*theta"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
