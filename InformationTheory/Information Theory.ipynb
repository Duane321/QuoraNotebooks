{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random notes:\n",
    "- A good working definition of 'intuitive explanation' is one that allows you to apply the concept without the mental overhead of all the mathematical details.\n",
    "- The attraction is how H(X,Y) breaks up into pieces (H(X|Y), I(X;Y), H(X), etc...)\n",
    "- Entropy doesn't care about *which* values.\n",
    "- 'Knowing' an RV is akin to 'subtracting' uncertainty\n",
    "- We will only talk about a discrete RV\n",
    "- I should address why high uncertainty is associated with high information. It's because a rare event is a surprise - it's good amount of information. High uncertainty is associtad with a high expectation of a surprise. \n",
    "- Entropy gives a limit on the average lenght of messages.. should mention that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame({r'$\\mathcal{X}$':[r'$x_1$',r'$x_2$',r'$\\vdots$',r'$x_N$'],\n",
    "                     'Probability':[r'$p(x_1)$',r'$p(x_2)$',r'$\\vdots$',r'$p(x_N)$']})\n",
    "dist[r'$i$'] = ['1','2',r'$\\vdots$','N']\n",
    "dist_names = [r'$i$',r'$\\mathcal{X}$','Probability']\n",
    "dist = dist[dist_names]\n",
    "\n",
    "N = 5\n",
    "vals = [x*5 for x in range(1,N+1)]\n",
    "\n",
    "X1 = pd.DataFrame({r'$\\mathcal{X}$':vals,\n",
    "                     'Probability':[1/N for x in vals]})\n",
    "X1[r'$i$'] = [x for x in range(1,N+1)]\n",
    "X1 = X1[dist_names]\n",
    "\n",
    "X2 = pd.DataFrame({r'$\\mathcal{X}$':vals,\n",
    "                     'Probability':[0 for x in vals]})\n",
    "X2.loc[3,'Probability'] = 1\n",
    "X2[r'$i$'] = [x for x in range(1,N+1)]\n",
    "X2 = X2[dist_names]\n",
    "\n",
    "def xlogx(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return x*np.log2(x)\n",
    "\n",
    "def H(x_probs):\n",
    "    return -np.sum([xlogx(x) for x in x_probs])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an intuitive explanation of the concept of entropy in information theory?\n",
    "\n",
    "A good intuitive explanation enables you to apply a concept without the mental overhead of all the mathematical details. Entropy is particularly friendly to such logical shortcuts. It measures a very important character of random variables, but at the same time, has an analogy that makes reasoning about it as easy as addition and subtraction.\n",
    "\n",
    "Let me show you what I mean. To start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is entropy?\n",
    "\n",
    "Let's consider entropy for some discrete random variable $X$ [1]. To do that, we'll have to think explicitly about $X$'s distribution - we say that $X$ can result in any value $x$ in the set $\\mathcal{X}$ with probability $p(x)$. If there are $N$ values in the set $\\mathcal{X}$, we can represent this distribution as a grid:\n",
    "\n",
    "![title](ExampleGrid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is simply a function of this grid with the goal of measuring $X$'s uncertainty [2]. If $X$ has high entropy, it is highly uncertain. It is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbb{H}(X) = -\\sum_{i=1}^{N} p(x_i) \\log_2 p(x_i)\n",
    "$$\n",
    "\n",
    "There are some clever interpretations of what this definition means, but for our purposes, those are best left as a footnote[3]. Entropy isn't remarkable for its interpretation, but for its *properties*. To get a handle on those, it's important first to understand what entropy implies about the grid and hence, the random variable.\n",
    "\n",
    "One thing to notice is that entropy doesn't care about the actual $x_i$ values. It only considers their probabilities $p(x_i)$. Contrast this with another measure of uncertainty, variance, which certainly cares about those $x_i$ values. Among other benefits, this lets us completely represent a grid with a bar graph whereby the horizontal axis is the index $i$ and the height of the bar is $p(x_i)$. We get a feel for entropy by observing it's value as we vary bar graphs (and hence distributions) along certain characteristics. I'll pick these two:\n",
    "\n",
    "1. Increase $N$. How does entropy respond as we increase the number of values $X$ may take?\n",
    "2. Decrease the 'concentration' of probability to certain values. So in one extreme, all $p(x_i)$'s have the same value and in the other, a few $x_i$'s contain the majority of the probability mass.\n",
    "\n",
    "One other thing to note is that entropy doesn't care about the *order* of our $x_i$'s. So we can always sort our bars from tallest to shortest without fear that messes with the entropy. So when you see a sorted bar graph, realize this represents all bar graphs that could be sorted into that. \n",
    "\n",
    "So let's look:\n",
    "\n",
    "![title](Entropy_behavior_adj2.png)\n",
    "\n",
    "So we see: entropy increases when there are more values for $X$ to take ($\\mathcal{X}$ is larger) and/or the probabilities are less concentrated. That's all that's happening.\n",
    "\n",
    "Now that we know what entropy is after, let's move on to the cool stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of the joint distribution\n",
    "\n",
    "Let's consider two random variables $X$ and $Y$ along with their joint distribution. So, more grids:\n",
    "\n",
    "![title](JointGrid.png)\n",
    "\n",
    "Now the question is, how does the entropy of the joint distribution, $\\mathbb{H}[X,Y]$, relate to the entropies $\\mathbb{H}[X]$ and $\\mathbb{H}[Y]$? To get our bearings, I will tell you two facts:\n",
    "\n",
    "1. If $X$ is a known one-to-one function of $Y$ (and therefore, $Y$ is a one-to-one function of $X$), then $\\mathbb{H}[X,Y]=\\mathbb{H}[X]=\\mathbb{H}[Y]$. Since the moment you know one, you know the other, the uncertainty of the joint distribution is just the uncertainty of either of them.\n",
    "2. If $X$ and $Y$ are independent, $\\mathbb{H}[X,Y]=\\mathbb{H}[X]+\\mathbb{H}[Y]$. The uncertainty of both is the sum of their individual uncertainties. [4]\n",
    "\n",
    "In a sense, these represent opposite extremes of what their joint behavior could be: either a perfect dependence or perfect independence. $\\mathbb{H}[X,Y]$ is just a single number, so let's represent it as an *area* (encased by red) in either of these extreme cases:\n",
    "\n",
    "![title](Extremes.png)\n",
    "\n",
    "This suggests a representation of the generic middle ground:\n",
    "\n",
    "![title](MiddleGroundLabeled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remarkable fact is these regions (labeled with some anticipatory notation) are, separately, quantities of significant interest. It's like opening your car's hood to see how the engine works, only to discover it's just a toaster inside a microwave inside a refrigator. How convenient it is that something so useful is built so simply from other useful things!\n",
    "\n",
    "### What are these important things?\n",
    "\n",
    "The leftmost region is the **conditional entropy** (link) of $X$ given $Y$. It's formula is:\n",
    "\n",
    "$$\n",
    "\\mathbb{H}[X|Y] = \\sum_{j=1}^{M} \\mathbb{H}(X|Y=y_j)p_Y(y_j)\n",
    "$$\n",
    "\n",
    "To understand this, imagine we sampled pairs $(x,y)$ from the joint distribution, but the $x$ value is hidden from us. If we use $y$ and our knowledge of their joint distribution, we can create a new distribution for $x$. So for each $y$-value, you can form a new $X$ distribution grid, each of which gets their own entropy. If you weight these entropies by the probability of their respective $p_Y(y)$ values, you get $\\mathbb{H}[X|Y]$. This is similarly true for $\\mathbb{H}[Y|X]$.\n",
    "\n",
    "What about the middle region? This is called the **mutual information** and it's given by\n",
    "\n",
    "$$\n",
    "\\mathbb{I}[X;Y] = \\sum_i^N \\sum_j^M p_{X,Y}(x_i,y_j) \\log\\big(\\frac{p_{X,Y}(x_i,y_j)}{p_X(x_i)p_Y(y_j)}\\big)\n",
    "$$\n",
    "\n",
    "To understand this, we'd first need to understand KL-divergence, which is worthy of it's own post. In simpliest terms, KL divergence is a psuedo-distance measure between two probability distributions. Identical distributions have a KL divergence of 0 and dissimilar ones have a large positive value. Mutual information is the KL-divergence between the joint distribution $p_{X,Y}(x_i,y_j)$ and an approximation of the joint made by a product of the marginal distributions, $p_X(x_i)p_Y(y_j)$. So if the mutual information is zero, then this product is a perfect approximation, meaning $p_{X,Y}(x_i,y_j) = p_X(x_i)p_Y(y_j)$, and therefore, $X$ and $Y$ are independent. If mutual information is high, then on average $p_{X,Y}(x_i,y_j)$ looks very different from $p_X(x_i)p_Y(y_j)$, so it's very-not-independent. You can tell that this behavior agrees with our visual - just imagine what happens as you shrink or grow the $\\mathbb{I}[X;Y]$ region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No mathematical overhead.\n",
    "\n",
    "At this point, we know $\\mathbb{H}[X,Y]$, $\\mathbb{H}[X]$, $\\mathbb{H}[Y]$, $\\mathbb{H}[Y|X]$, $\\mathbb{H}[X|Y]$ and $\\mathbb{I}[X;Y]$ are all independently valuable (or at least, I ask you take my word on that). Now, look at their formulas separately. Imagine trying to do manipulations with only those in mind. Hard right? They don't *look* like they combine in any obvious way. Mixing and matching these expressions from here is, at the very least, a slow process.\n",
    "\n",
    "But the middle-ground visual tells us these quantities relate to each other in the same super simple way **sets** do. Knowing this set-analogy allows us to avoid all that nasty algebra. I can simply *think* about those sets and produce true statements:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{H}[X,Y] & = \\mathbb{H}[X] + \\mathbb{H}[Y] - \\mathbb{I}[X;Y] \\\\\n",
    "\\mathbb{H}[X,Y] & = \\mathbb{H}[X] + \\mathbb{H}[Y|X] \\\\\n",
    "\\mathbb{H}[X,Y] & = \\mathbb{H}[X|Y] + \\mathbb{H}[Y|X] + \\mathbb{I}[X;Y] \\\\\n",
    "\\mathbb{I}[X;Y] & = \\mathbb{H}[Y] - \\mathbb{H}[Y|X] \\\\\n",
    "\\mathbb{H}[X] & \\geq \\mathbb{H}[X|Y] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And we can keep going - how awesome is that? This set analogy gives us so much agility with these quantities.\n",
    "\n",
    "Also, we can easily attach an intuition to this set-analogy. To *know* a random variable (as you know $Y$ for $\\mathbb{H}[X|Y]$) is to *subtract* off the uncertainty from $\\mathbb{H}[X]$. How much do you subtract off? The amount of uncertainty created by their dependency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up.\n",
    "\n",
    "So there you have it. Entropy measures uncertainty well and comes with this elegant and simple decomposition into important pieces. Knowing this particular angle can make the subject feel easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[1] All the core ideas translate seemlessly to continuous random variables. All we need to do is replace sums with integrals.\n",
    "\n",
    "[2] Strangely, 'uncertainty' and 'information' mean the same thing in this domain. To see this, we first characterize what an 'uninformative' event is. An uninformative event is a likely event - an event you expect to happen. Since you expect it to happen, you (to a degree) already *know* that it'll happen. Therefore, you already *have* the information of the event. By the same logic, an informative event is a *rare* event. So a random variable is very informative when it's average outcome is rare, and therefore, hard to predict. A hard to predict random variable is very *uncertain*.\n",
    "\n",
    "[3] The interpretation I like most is one closely related to entropy's original motivation. Imagine we are to repeatedly sample $x_i$'s and our job is to communicate each across a noiseless channel using an encoding of 0's and 1's. For example, we may encode $x_i=$ 'Quora' as '0001'. If we know the distribution $p_X(\\cdot)$ beforehand, what is the smallest average bit-length per $x_i$ we can hope to achieve? $\\mathbb{H}[X]$ gives us the shortest achievable bit length.\n",
    "\n",
    "[4] We can see this immediately if we apply the definition of independence ($p_{X,Y}(x_i,y_j)=p_X(x_i)p_Y(y_j)$) to the joint grid and apply the entropy formula.\n",
    "\n",
    "### Sources\n",
    "\n",
    "[1] Wikipedia\n",
    "[2] Scholarpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
